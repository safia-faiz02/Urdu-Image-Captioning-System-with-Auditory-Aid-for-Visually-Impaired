{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7980273,"sourceType":"datasetVersion","datasetId":4696823},{"sourceId":8564412,"sourceType":"datasetVersion","datasetId":5120042},{"sourceId":8575135,"sourceType":"datasetVersion","datasetId":5127626}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"# !pip install nltk\n# !pip install opencv-python ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-06-01T05:01:11.501327Z","iopub.execute_input":"2024-06-01T05:01:11.501657Z","iopub.status.idle":"2024-06-01T05:01:11.506387Z","shell.execute_reply.started":"2024-06-01T05:01:11.501632Z","shell.execute_reply":"2024-06-01T05:01:11.505423Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom textwrap import wrap\n\nimport os\nimport gc       # garbage collection: for memory allocation and deallocation\nimport pickle\nimport keras\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model, Sequence\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, BatchNormalization, Concatenate\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# from nltk.translate.bleu_score import corpus_bleu\n\nfrom PIL import Image\n\n# import cv2\n# from ultralytics import YOLO","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:45:36.665424Z","iopub.execute_input":"2024-06-05T06:45:36.665752Z","iopub.status.idle":"2024-06-05T06:45:49.476593Z","shell.execute_reply.started":"2024-06-05T06:45:36.665725Z","shell.execute_reply":"2024-06-05T06:45:49.475817Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-05 06:45:39.003690: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 06:45:39.003792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 06:45:39.094663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"BLEU score implementation.\"\"\"\nimport math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\n\nfrom nltk.util import ngrams\n\n\nclass Fraction(_Fraction):\n    \"\"\"Fraction with _normalize=False support for 3.12\"\"\"\n\n    def __new__(cls, numerator=0, denominator=None, _normalize=False):\n        if sys.version_info >= (3, 12):\n            self = super().__new__(cls, numerator, denominator)\n        else:\n            self = super().__new__(cls, numerator, denominator, _normalize=_normalize)\n        self._normalize = _normalize\n        self._original_numerator = numerator\n        self._original_denominator = denominator\n        return self\n\n    @property\n    def numerator(self):\n        if not self._normalize:\n            return self._original_numerator\n        return super().numerator\n\n    @property\n    def denominator(self):\n        if not self._normalize:\n            return self._original_denominator\n        return super().denominator\n\ndef sentence_bleu(\n    references,\n    hypothesis,\n    weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None,\n    auto_reweigh=False,):\n    return corpus_bleu([references], [hypothesis], weights, smoothing_function, auto_reweigh)\n\ndef corpus_bleu(\n    list_of_references,\n    hypotheses,\n    weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None,\n    auto_reweigh=False,):\n    # Before proceeding to compute BLEU, perform sanity checks.\n\n    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n    hyp_lengths, ref_lengths = 0, 0\n\n    assert len(list_of_references) == len(hypotheses), ()\n\n    try:\n        weights[0][0]\n    except:\n        weights = [weights]\n    max_weight_length = max(len(weight) for weight in weights)\n\n    # Iterate through each hypothesis and their corresponding references.\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        # For each order of ngram, calculate the numerator and\n        # denominator for the corpus-level modified precision.\n        for i in range(1, max_weight_length + 1):\n            p_i = modified_precision(references, hypothesis, i)\n            p_numerators[i] += p_i.numerator\n            p_denominators[i] += p_i.denominator\n\n        # Calculate the hypothesis length and the closest reference length.\n        # Adds them to the corpus-level hypothesis and reference counts.\n        hyp_len = len(hypothesis)\n        hyp_lengths += hyp_len\n        ref_lengths += closest_ref_length(references, hyp_len)\n\n    # Calculate corpus-level brevity penalty.\n    bp = brevity_penalty(ref_lengths, hyp_lengths)\n\n    # Collects the various precision values for the different ngram orders.\n    p_n = [Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n        for i in range(1, max_weight_length + 1)]\n\n    # Returns 0 if there's no matching n-grams\n    # We only need to check for p_numerators[1] == 0, since if there's\n    # no unigrams, there won't be any higher order ngrams.\n    if p_numerators[1] == 0:\n        return 0 if len(weights) == 1 else [0] * len(weights)\n\n    # If there's no smoothing, set use method0 from SmoothinFunction class.\n    if not smoothing_function:\n        smoothing_function = SmoothingFunction().method0\n    # Smoothen the modified precision.\n    # Note: smoothing_function() may convert values into floats;\n    #       it tries to retain the Fraction object as much as the\n    #       smoothing method allows.\n    p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths)\n\n    bleu_scores = []\n    for weight in weights:\n        # Uniformly re-weighting based on maximum hypothesis lengths if largest\n        # order of n-grams < 4 and weights is set at default.\n        if auto_reweigh:\n            if hyp_lengths < 4 and weight == (0.25, 0.25, 0.25, 0.25):\n                weight = (1 / hyp_lengths,) * hyp_lengths\n\n        s = (w_i * math.log(p_i) for w_i, p_i in zip(weight, p_n) if p_i > 0)\n        s = bp * math.exp(math.fsum(s))\n        bleu_scores.append(s)\n    return bleu_scores[0] if len(weights) == 1 else bleu_scores\n\n\ndef modified_precision(references, hypothesis, n):\n    # Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (Counter(ngrams(reference, n)) if len(reference) >= n else Counter())\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()}\n\n    numerator = sum(clipped_counts.values())\n    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n    # Usually this happens when the ngram order is > len(reference).\n    denominator = max(1, sum(counts.values()))\n\n    return Fraction(numerator, denominator, _normalize=False)\n\n\ndef closest_ref_length(references, hyp_len):\n    ref_lens = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))\n    return closest_ref_len\n\n\ndef brevity_penalty(closest_ref_len, hyp_len):\n    if hyp_len > closest_ref_len:\n        return 1\n    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n    elif hyp_len == 0:\n        return 0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)\n\n\nclass SmoothingFunction:\n    def __init__(self, epsilon=0.1, alpha=5, k=5):\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.k = k\n\n    def method0(self, p_n, *args, **kwargs):\n        \"\"\"\n        No smoothing.\n        \"\"\"\n        p_n_new = []\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator != 0:\n                p_n_new.append(p_i)\n            else:\n                _msg = str().format(i + 1)\n                warnings.warn(_msg)\n                # When numerator==0 where denonminator==0 or !=0, the result\n                # for the precision score should be equal to 0 or undefined.\n                # Due to BLEU geometric mean computation in logarithm space,\n                # we we need to take the return sys.float_info.min such that\n                # math.log(sys.float_info.min) returns a 0 precision score.\n                p_n_new.append(sys.float_info.min)\n        return p_n_new\n\n    def method1(self, p_n, *args, **kwargs):\n        \"\"\"\n        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n        \"\"\"\n        return [(p_i.numerator + self.epsilon) / p_i.denominator\n            if p_i.numerator == 0\n            else p_i\n            for p_i in p_n]\n\n    def method2(self, p_n, *args, **kwargs):\n        return [Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n            if i != 0\n            else p_n[0]\n            for i in range(len(p_n))]\n\n    def method3(self, p_n, *args, **kwargs):\n        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0:\n                p_n[i] = 1 / (2**incvnt * p_i.denominator)\n                incvnt += 1\n        return p_n\n\n    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        incvnt = 1\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0 and hyp_len > 1:\n                # incvnt = i + 1 * self.k / math.log(\n                #     hyp_len\n                # )  # Note that this K is different from the K from NIST.\n                # p_n[i] = incvnt / p_i.denominator\\\n                numerator = 1 / (2**incvnt * self.k / math.log(hyp_len))\n                p_n[i] = numerator / p_i.denominator\n                incvnt += 1\n        return p_n\n\n    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        m = {}\n        # Requires an precision value for an addition ngram order.\n        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n        m[-1] = p_n[0] + 1\n        for i, p_i in enumerate(p_n):\n            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n            m[i] = p_n[i]\n        return p_n\n\n    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        # This smoothing only works when p_1 and p_2 is non-zero.\n        # Raise an error with an appropriate message when the input is too short\n        # to use this smoothing technique.\n        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n        for i, p_i in enumerate(p_n):\n            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n                continue\n            else:\n                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n                # No. of ngrams in translation that matches the reference.\n                m = p_i.numerator\n                # No. of ngrams in translation.\n                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n                # Calculates the interpolated precision.\n                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n        return p_n\n\n    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        \"\"\"\n        Smoothing method 7:\n        Interpolates methods 4 and 5.\n        \"\"\"\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n        return p_n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:45:49.478464Z","iopub.execute_input":"2024-06-05T06:45:49.479264Z","iopub.status.idle":"2024-06-05T06:45:50.015874Z","shell.execute_reply.started":"2024-06-05T06:45:49.479229Z","shell.execute_reply":"2024-06-05T06:45:50.014934Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## For GPU","metadata":{}},{"cell_type":"code","source":"strategy = tf.distribute.MultiWorkerMirroredStrategy()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:45:50.017220Z","iopub.execute_input":"2024-06-05T06:45:50.017524Z","iopub.status.idle":"2024-06-05T06:45:50.984975Z","shell.execute_reply.started":"2024-06-05T06:45:50.017500Z","shell.execute_reply":"2024-06-05T06:45:50.984031Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## For TPU","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.TPUStrategy(tpu)\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T13:06:55.782127Z","iopub.execute_input":"2024-05-07T13:06:55.783009Z","iopub.status.idle":"2024-05-07T13:07:04.481206Z","shell.execute_reply.started":"2024-05-07T13:06:55.782975Z","shell.execute_reply":"2024-05-07T13:07:04.480427Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Data ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataset4cap/FINAL_COMBINED_DATASET_4CAP.csv')\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:49:45.551832Z","iopub.execute_input":"2024-06-05T06:49:45.552553Z","iopub.status.idle":"2024-06-05T06:49:50.684880Z","shell.execute_reply.started":"2024-06-05T06:49:45.552521Z","shell.execute_reply":"2024-06-05T06:49:50.683860Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"           image                                            caption  \\\n0  img000000.jpg  Closeup of bins of food that include broccoli ...   \n1  img000000.jpg         A bunch of trays that have different food.   \n2  img000000.jpg  Colorful dishes holding meat, vegetables, frui...   \n3  img000000.jpg  there are containers filled with different kin...   \n4  img000001.jpg               A giraffe standing up nearby a tree    \n5  img000001.jpg      A giraffe mother with its baby in the forest.   \n6  img000001.jpg       Two giraffes standing in a tree filled area.   \n7  img000001.jpg  A giraffe standing next to a forest filled wit...   \n8  img000002.jpg  White vase with different colored flowers sitt...   \n9  img000002.jpg         A flower vase is sitting on a porch stand.   \n\n                                        urdu_caption  \n0  کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی اور رو...  \n1       ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے ہیں۔  \n2  رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھل اور ر...  \n3             مختلف قسم کے کھانے سے بھرے کنٹینر ہیں۔  \n4                ایک زرافہ ایک درخت کے قریب کھڑا ہے۔  \n5        ایک زرافے کی ماں اپنے بچے کے ساتھ جنگل میں۔  \n6        دو زرافے درختوں سے بھرے علاقے میں کھڑے ہیں۔  \n7      ایک زرافہ درختوں سے بھرے جنگل کے پاس کھڑا ہے۔  \n8  سفید گلدان جس کے اندر مختلف رنگوں کے پھول بیٹھ...  \n9           پورچ اسٹینڈ پر پھولوں کا گلدان بیٹھا ہے۔  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>urdu_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img000000.jpg</td>\n      <td>Closeup of bins of food that include broccoli ...</td>\n      <td>کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی اور رو...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img000000.jpg</td>\n      <td>A bunch of trays that have different food.</td>\n      <td>ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے ہیں۔</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img000000.jpg</td>\n      <td>Colorful dishes holding meat, vegetables, frui...</td>\n      <td>رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھل اور ر...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img000000.jpg</td>\n      <td>there are containers filled with different kin...</td>\n      <td>مختلف قسم کے کھانے سے بھرے کنٹینر ہیں۔</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe standing up nearby a tree</td>\n      <td>ایک زرافہ ایک درخت کے قریب کھڑا ہے۔</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe mother with its baby in the forest.</td>\n      <td>ایک زرافے کی ماں اپنے بچے کے ساتھ جنگل میں۔</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>img000001.jpg</td>\n      <td>Two giraffes standing in a tree filled area.</td>\n      <td>دو زرافے درختوں سے بھرے علاقے میں کھڑے ہیں۔</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe standing next to a forest filled wit...</td>\n      <td>ایک زرافہ درختوں سے بھرے جنگل کے پاس کھڑا ہے۔</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>img000002.jpg</td>\n      <td>White vase with different colored flowers sitt...</td>\n      <td>سفید گلدان جس کے اندر مختلف رنگوں کے پھول بیٹھ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>img000002.jpg</td>\n      <td>A flower vase is sitting on a porch stand.</td>\n      <td>پورچ اسٹینڈ پر پھولوں کا گلدان بیٹھا ہے۔</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Display Images with Captions","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# def readImage(path, img_size = 224):\n#     img = load_img(path, color_mode = 'rgb', target_size = (img_size, img_size))\n#     img = img_to_array(img)\n#     img = img/255.\n#     return img\n\n# # THIS IS NOT NECESSARY, URDU CAPTIONS THEK DISPLAY NH HO RHE\n# def display_images(temp_df):   \n#     temp_df = temp_df.reset_index(drop = True)\n#     plt.figure(figsize = (20, 20))\n#     n = 0\n#     for i in range(156070 * 5):  \n#         n += 1\n#         plt.subplot(156070, 5, n)  \n#         plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n\n#         image = readImage(f\"/kaggle/input/images/Images/{temp_df.image[i]}\")\n#         plt.imshow(image)\n#         plt.title(\"\\n\".join(wrap(temp_df.urdu_caption[i][-1::-1], 20)))\n#         plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:32:56.758119Z","iopub.execute_input":"2024-04-27T20:32:56.758432Z","iopub.status.idle":"2024-04-27T20:32:56.762586Z","shell.execute_reply.started":"2024-04-27T20:32:56.758393Z","shell.execute_reply":"2024-04-27T20:32:56.761906Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display_images(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:32:56.763398Z","iopub.execute_input":"2024-04-27T20:32:56.763624Z","iopub.status.idle":"2024-04-27T20:32:56.775319Z","shell.execute_reply.started":"2024-04-27T20:32:56.7636Z","shell.execute_reply":"2024-04-27T20:32:56.774593Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model to Extract Features","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# Load the Model\ncnn = YOLO(\"yolov8m-cls.pt\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-30T17:55:18.866794Z","iopub.execute_input":"2024-05-30T17:55:18.867487Z","iopub.status.idle":"2024-05-30T17:55:20.875800Z","shell.execute_reply.started":"2024-05-30T17:55:18.867453Z","shell.execute_reply":"2024-05-30T17:55:20.874935Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Image Features","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    features = {}\n    directory = '/kaggle/input/images/Images'\n\n    for img_name in tqdm(os.listdir(directory)): \n        img_path = directory + '/' + img_name\n        feature = cnn.predict(img_path, embed = [-1])    # extract features\n        image_id = img_name.split('.')[0]             # get image ID\n        features[image_id] = feature[0]               # store feature  (size = 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T16:38:34.442173Z","iopub.execute_input":"2024-05-30T16:38:34.442863Z","iopub.status.idle":"2024-05-30T16:50:05.222610Z","shell.execute_reply.started":"2024-05-30T16:38:34.442831Z","shell.execute_reply":"2024-05-30T16:50:05.221630Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store features in pickle\npickle.dump(features, open('ImgFeaturesYOLO.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T16:51:12.026871Z","iopub.execute_input":"2024-05-30T16:51:12.027268Z","iopub.status.idle":"2024-05-30T16:51:22.001991Z","shell.execute_reply.started":"2024-05-30T16:51:12.027237Z","shell.execute_reply":"2024-05-30T16:51:22.001140Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load features from pickle\nwith open('/kaggle/input/yolopkl/ImgFeaturesYOLO.pkl', 'rb') as f:    \n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:46:22.078918Z","iopub.execute_input":"2024-06-05T06:46:22.079776Z","iopub.status.idle":"2024-06-05T06:46:59.978171Z","shell.execute_reply.started":"2024-06-05T06:46:22.079744Z","shell.execute_reply":"2024-06-05T06:46:59.977156Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# features","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-01T05:02:24.325078Z","iopub.execute_input":"2024-06-01T05:02:24.325605Z","iopub.status.idle":"2024-06-01T05:02:24.329443Z","shell.execute_reply.started":"2024-06-01T05:02:24.325579Z","shell.execute_reply":"2024-06-01T05:02:24.328564Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# features['img000000'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-01T05:02:24.330529Z","iopub.execute_input":"2024-06-01T05:02:24.330782Z","iopub.status.idle":"2024-06-01T05:02:24.342621Z","shell.execute_reply.started":"2024-06-01T05:02:24.330761Z","shell.execute_reply":"2024-06-01T05:02:24.341704Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess captions","metadata":{}},{"cell_type":"code","source":"def preprocessCaption(df):\n    df['urdu_caption'] = 'endseq ' + df['urdu_caption'] + ' startseq'\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:46:59.989304Z","iopub.execute_input":"2024-06-05T06:46:59.989859Z","iopub.status.idle":"2024-06-05T06:46:59.993917Z","shell.execute_reply.started":"2024-06-05T06:46:59.989832Z","shell.execute_reply":"2024-06-05T06:46:59.992998Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = data.apply(preprocessCaption, axis = 1)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:49:56.871068Z","iopub.execute_input":"2024-06-05T06:49:56.871411Z","iopub.status.idle":"2024-06-05T06:50:42.971454Z","shell.execute_reply.started":"2024-06-05T06:49:56.871380Z","shell.execute_reply":"2024-06-05T06:50:42.970476Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                image                                            caption  \\\n0       img000000.jpg  Closeup of bins of food that include broccoli ...   \n1       img000000.jpg         A bunch of trays that have different food.   \n2       img000000.jpg  Colorful dishes holding meat, vegetables, frui...   \n3       img000000.jpg  there are containers filled with different kin...   \n4       img000001.jpg               A giraffe standing up nearby a tree    \n...               ...                                                ...   \n624275  img155818.jpg              an owl perched on a branch in a tree    \n624276  img155818.jpg                        an owl sitting on a branch    \n624277  img155819.jpg  a small brown elephant with brown fur eating g...   \n624278  img155819.jpg  a brown horse eating grass with a green backgr...   \n624279  img155819.jpg  a brown and white horse eating grass with its ...   \n\n                                             urdu_caption  \n0       endseq کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی...  \n1       endseq ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے...  \n2       endseq رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھ...  \n3       endseq مختلف قسم کے کھانے سے بھرے کنٹینر ہیں۔ ...  \n4       endseq ایک زرافہ ایک درخت کے قریب کھڑا ہے۔ sta...  \n...                                                   ...  \n624275  endseq ایک اُلّو درخت کی شاخ پر بیٹھا ہے۔ star...  \n624276              endseq شاخ پر بیٹھا ہوا الّو startseq  \n624277  endseq بھوری کھال کے ساتھ ایک چھوٹا بھورا ہاتھ...  \n624278  endseq ایک بھورا گھوڑا سبز پس منظر کے ساتھ گھا...  \n624279  endseq ایک بھورا اور سفید گھوڑا اپنے سر کے سات...  \n\n[624280 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>urdu_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img000000.jpg</td>\n      <td>Closeup of bins of food that include broccoli ...</td>\n      <td>endseq کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img000000.jpg</td>\n      <td>A bunch of trays that have different food.</td>\n      <td>endseq ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img000000.jpg</td>\n      <td>Colorful dishes holding meat, vegetables, frui...</td>\n      <td>endseq رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img000000.jpg</td>\n      <td>there are containers filled with different kin...</td>\n      <td>endseq مختلف قسم کے کھانے سے بھرے کنٹینر ہیں۔ ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe standing up nearby a tree</td>\n      <td>endseq ایک زرافہ ایک درخت کے قریب کھڑا ہے۔ sta...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>624275</th>\n      <td>img155818.jpg</td>\n      <td>an owl perched on a branch in a tree</td>\n      <td>endseq ایک اُلّو درخت کی شاخ پر بیٹھا ہے۔ star...</td>\n    </tr>\n    <tr>\n      <th>624276</th>\n      <td>img155818.jpg</td>\n      <td>an owl sitting on a branch</td>\n      <td>endseq شاخ پر بیٹھا ہوا الّو startseq</td>\n    </tr>\n    <tr>\n      <th>624277</th>\n      <td>img155819.jpg</td>\n      <td>a small brown elephant with brown fur eating g...</td>\n      <td>endseq بھوری کھال کے ساتھ ایک چھوٹا بھورا ہاتھ...</td>\n    </tr>\n    <tr>\n      <th>624278</th>\n      <td>img155819.jpg</td>\n      <td>a brown horse eating grass with a green backgr...</td>\n      <td>endseq ایک بھورا گھوڑا سبز پس منظر کے ساتھ گھا...</td>\n    </tr>\n    <tr>\n      <th>624279</th>\n      <td>img155819.jpg</td>\n      <td>a brown and white horse eating grass with its ...</td>\n      <td>endseq ایک بھورا اور سفید گھوڑا اپنے سر کے سات...</td>\n    </tr>\n  </tbody>\n</table>\n<p>624280 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.iloc[8714, 2]","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:50:42.973556Z","iopub.execute_input":"2024-06-05T06:50:42.974335Z","iopub.status.idle":"2024-06-05T06:50:42.980214Z","shell.execute_reply.started":"2024-06-05T06:50:42.974298Z","shell.execute_reply":"2024-06-05T06:50:42.979244Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'endseq لوگوں کا ایک گروپ ایک میز کے گرد بیٹھا ہے۔ startseq'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing the Text","metadata":{}},{"cell_type":"markdown","source":"In Python, indices typically start from 0. However, when working with tokenization in NLP, we often reserve index 0 for special tokens, such as padding tokens or unknown tokens.\n* **Padding Token:** In many NLP tasks, sequences of words or tokens are padded to ensure uniform length. Padding tokens are used to fill in the extra spaces in sequences to make them uniform. Index 0 is usually reserved for the padding token.\n<br><br>\n* **Unknown Token:** This token is used to represent words that are not present in the vocabulary. When a word that is not in the vocabulary is encountered during tokenization, it is replaced by the unknown token. Again, index 0 is often reserved for this purpose.","metadata":{}},{"cell_type":"code","source":"# tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data[\"urdu_caption\"])\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:50:42.981295Z","iopub.execute_input":"2024-06-05T06:50:42.981588Z","iopub.status.idle":"2024-06-05T06:50:58.960656Z","shell.execute_reply.started":"2024-06-05T06:50:42.981563Z","shell.execute_reply":"2024-06-05T06:50:58.959773Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"29872"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.word_index","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-05T06:50:58.963176Z","iopub.execute_input":"2024-06-05T06:50:58.963841Z","iopub.status.idle":"2024-06-05T06:50:59.000960Z","shell.execute_reply.started":"2024-06-05T06:50:58.963804Z","shell.execute_reply":"2024-06-05T06:50:59.000027Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'endseq': 1,\n 'startseq': 2,\n 'ایک': 3,\n 'کے': 4,\n 'ہے۔': 5,\n 'میں': 6,\n 'پر': 7,\n 'اور': 8,\n 'رہا': 9,\n 'کی': 10,\n 'ہیں۔': 11,\n 'ساتھ': 12,\n 'کا': 13,\n 'سے': 14,\n 'آدمی': 15,\n 'کر': 16,\n 'رہی': 17,\n 'دو': 18,\n 'رہے': 19,\n 'کو': 20,\n 'ہے': 21,\n 'ہوئے': 22,\n 'جس': 23,\n 'عورت': 24,\n 'سفید': 25,\n 'ہوا': 26,\n 'بیٹھا': 27,\n 'کھڑا': 28,\n 'سڑک': 29,\n 'جو': 30,\n 'میز': 31,\n 'بورڈ': 32,\n 'اپنے': 33,\n 'کچھ': 34,\n 'اوپر': 35,\n 'لوگ': 36,\n 'شخص': 37,\n 'پہنے': 38,\n 'بال': 39,\n 'رنگ': 40,\n 'کھیل': 41,\n 'سامنے': 42,\n 'دیکھ': 43,\n 'سرخ': 44,\n 'نوجوان': 45,\n 'لیے': 46,\n 'سوار': 47,\n 'گروپ': 48,\n 'پکڑے': 49,\n 'والی': 50,\n 'پاس': 51,\n 'قریب': 52,\n 'نیچے': 53,\n 'والے': 54,\n 'پانی': 55,\n 'لوگوں': 56,\n 'والا': 57,\n 'کھڑی': 58,\n 'چل': 59,\n 'گھاس': 60,\n 'بیٹھی': 61,\n 'ٹینس': 62,\n 'کھڑے': 63,\n 'ہوئی': 64,\n 'بہت': 65,\n 'لڑکا': 66,\n 'اس': 67,\n 'سیاہ': 68,\n 'ہیں': 69,\n 'باہر': 70,\n 'بیس': 71,\n 'چھوٹا': 72,\n 'پلیٹ': 73,\n 'تصویر': 74,\n 'بیٹھے': 75,\n 'بلی': 76,\n 'قمیض': 77,\n 'کتا': 78,\n 'لڑکی': 79,\n 'میدان': 80,\n 'ٹرین': 81,\n 'تین': 82,\n 'دوسرے': 83,\n 'سمندر': 84,\n 'طرف': 85,\n 'سبز': 86,\n 'عمارت': 87,\n 'سائیکل': 88,\n 'ساحل': 89,\n 'بڑا': 90,\n 'بچہ': 91,\n 'مرد': 92,\n 'روم': 93,\n 'فٹ': 94,\n 'کھانے': 95,\n 'باتھ': 96,\n 'نیلے': 97,\n 'برف': 98,\n 'کھلاڑی': 99,\n 'پیچھے': 100,\n 'ٹاپ': 101,\n 'لکڑی': 102,\n 'موٹر': 103,\n 'منظر': 104,\n 'کئی': 105,\n 'فون': 106,\n 'بڑی': 107,\n 'چھوٹی': 108,\n 'شہر': 109,\n 'اپنی': 110,\n 'بچے': 111,\n 'بس': 112,\n 'کمرے': 113,\n 'پیزا': 114,\n 'کنارے': 115,\n 'لباس': 116,\n 'نے': 117,\n 'ہوائی': 118,\n 'اسکیٹ': 119,\n 'بڑے': 120,\n 'گیند': 121,\n 'باورچی': 122,\n 'چھوٹے': 123,\n 'نشان': 124,\n 'لگا': 125,\n 'نیلی': 126,\n 'دیوار': 127,\n 'گلی': 128,\n 'لیپ': 129,\n 'سرف': 130,\n 'کرنے': 131,\n 'بستر': 132,\n 'مختلف': 133,\n 'سوٹ': 134,\n 'جا': 135,\n 'لے': 136,\n 'کتے': 137,\n 'ہاتھ': 138,\n 'کھا': 139,\n 'جبکہ': 140,\n 'پیلے': 141,\n 'پس': 142,\n 'درخت': 143,\n 'ملبوس': 144,\n 'ہو': 145,\n 'پارک': 146,\n 'جب': 147,\n 'کیک': 148,\n 'کھانا': 149,\n 'کمپیوٹر': 150,\n 'ہے،': 151,\n 'زیبرا': 152,\n 'ٹوپی': 153,\n 'ٹرک': 154,\n 'ہاتھی': 155,\n 'چھتری': 156,\n 'گھوڑے': 157,\n 'سکیٹ': 158,\n 'کالی': 159,\n 'کھیت': 160,\n 'فریسبی': 161,\n 'خواتین': 162,\n 'کھڑکی': 163,\n 'جہاز': 164,\n 'خاتون': 165,\n 'بینچ': 166,\n 'سکی': 167,\n 'چھلانگ': 168,\n 'گیا': 169,\n 'پاتھ': 170,\n 'سیل': 171,\n 'گزر': 172,\n 'گرد': 173,\n 'گھر': 174,\n 'خانے': 175,\n 'بالوں': 176,\n 'درختوں': 177,\n 'اندر': 178,\n 'علاقے': 179,\n 'باڑ': 180,\n 'سنک': 181,\n 'تیار': 182,\n 'کرتے': 183,\n 'فرش': 184,\n 'گھڑی': 185,\n 'پہاڑی': 186,\n 'کشتی': 187,\n 'آسمان': 188,\n 'پتنگ': 189,\n 'نارنجی': 190,\n 'سی': 191,\n 'یہ': 192,\n 'کار': 193,\n 'فائر': 194,\n 'زمین': 195,\n 'کسی': 196,\n 'سامان': 197,\n 'کورٹ': 198,\n 'وہ': 199,\n 'بات': 200,\n 'گلابی': 201,\n 'کرسی': 202,\n 'ٹوائلٹ': 203,\n 'اپنا': 204,\n 'بھرا': 205,\n 'چار': 206,\n 'بھرے': 207,\n 'سر': 208,\n 'ریکیٹ': 209,\n 'کام': 210,\n 'استعمال': 211,\n 'گاڑی': 212,\n 'لڑکے': 213,\n 'تصویر۔': 214,\n 'دن': 215,\n 'شیشے': 216,\n 'ریچھ': 217,\n 'سنو': 218,\n 'کاؤنٹر': 219,\n 'وے': 220,\n 'ہی': 221,\n 'ٹی': 222,\n 'صوفے': 223,\n 'پھولوں': 224,\n 'ٹریفک': 225,\n 'جنگل': 226,\n 'جیکٹ': 227,\n 'زرافے': 228,\n 'کہ': 229,\n 'بلے': 230,\n 'کرتا': 231,\n 'نظر': 232,\n 'ہجوم': 233,\n 'وقت': 234,\n 'چلا': 235,\n 'صاف': 236,\n 'ٹیڈی': 237,\n 'کیلے': 238,\n 'جوڑا': 239,\n 'گروپ۔': 240,\n 'پارکنگ': 241,\n 'بھری': 242,\n 'بیچ': 243,\n 'دے': 244,\n 'اڑ': 245,\n 'بنا': 246,\n 'بھورے': 247,\n 'کیمرے': 248,\n 'کھینچ': 249,\n 'ہیں،': 250,\n 'پٹریوں': 251,\n 'ٹائی': 252,\n 'زرافہ': 253,\n 'بیئر': 254,\n 'انتظار': 255,\n 'بورڈر': 256,\n 'شرٹ': 257,\n 'سینڈوچ': 258,\n 'آدمی۔': 259,\n 'اٹھائے': 260,\n 'درمیان': 261,\n 'شراب': 262,\n 'ہوتا': 263,\n 'گیم': 264,\n 'سرمئی': 265,\n 'دھوپ': 266,\n 'قریبی': 267,\n 'مسکرا': 268,\n 'بھورا': 269,\n 'پوز': 270,\n 'بیت': 271,\n 'اپ': 272,\n 'دکھا': 273,\n 'قسم': 274,\n 'پینٹ': 275,\n 'بیگ': 276,\n 'لہر': 277,\n 'موٹرسائیکل': 278,\n 'ان': 279,\n 'ڈھلوان': 280,\n 'لیٹی': 281,\n 'پکڑ': 282,\n 'ہائیڈرنٹ': 283,\n 'دوران': 284,\n 'اسٹیشن': 285,\n 'بجا': 286,\n 'ہوتے': 287,\n 'کوشش': 288,\n 'ٹاور': 289,\n 'بچوں': 290,\n 'جوڑے': 291,\n 'منہ': 292,\n 'دوڑ': 293,\n 'سفر': 294,\n 'ڈھکی': 295,\n 'پرندہ': 296,\n 'جاتا': 297,\n 'کیا': 298,\n 'کاٹ': 299,\n 'دروازے': 300,\n 'گائے': 301,\n 'ٹکڑا': 302,\n 'رات': 303,\n 'ارد': 304,\n 'خوبصورت': 305,\n 'سبزیوں': 306,\n 'کپ': 307,\n 'خالی': 308,\n 'کیس': 309,\n 'اینٹوں': 310,\n 'آئینے': 311,\n 'کھلے': 312,\n 'رہنے': 313,\n 'لمبی': 314,\n 'اونچی': 315,\n 'خانہ': 316,\n 'دوسری': 317,\n 'سیٹ': 318,\n 'کھمبے': 319,\n 'پہاڑ': 320,\n 'رکھا': 321,\n 'قطار': 322,\n 'ہاٹ': 323,\n 'سب': 324,\n 'ٹیلی': 325,\n 'بھوری': 326,\n 'سا': 327,\n 'کمرہ': 328,\n 'طرح': 329,\n 'مصروف': 330,\n 'پرانی': 331,\n 'چر': 332,\n 'لائٹ': 333,\n 'اڈے': 334,\n 'رنگین': 335,\n 'جسم': 336,\n 'چند': 337,\n 'ٹیک': 338,\n 'مردوں': 339,\n 'جھول': 340,\n 'کونے': 341,\n 'لیٹا': 342,\n 'کچن': 343,\n 'منظر۔': 344,\n 'آرام': 345,\n 'بروکولی': 346,\n 'پیالے': 347,\n 'کافی': 348,\n 'جانور': 349,\n 'سٹاپ': 350,\n 'راستے': 351,\n 'دوسرا': 352,\n 'ویڈیو': 353,\n 'پار': 354,\n 'کوئی': 355,\n 'ایشیائی': 356,\n 'گلدان': 357,\n 'ڈبل': 358,\n 'لڑکیاں': 359,\n 'دار': 360,\n 'اسٹاپ': 361,\n 'سو': 362,\n 'گھوڑا': 363,\n 'پی': 364,\n 'طیارہ': 365,\n 'مسافر': 366,\n 'بوڑھا': 367,\n 'ریت': 368,\n 'ڈاگ': 369,\n 'وی': 370,\n 'چیز': 371,\n 'بالغ': 372,\n 'جگہ': 373,\n 'گھوڑوں': 374,\n 'پیش': 375,\n 'کرتی': 376,\n 'ہیلمٹ': 377,\n 'روشنی': 378,\n 'سکینگ': 379,\n 'ریل': 380,\n 'پیلی': 381,\n 'کالا': 382,\n 'اڑا': 383,\n 'گندگی': 384,\n 'جامنی': 385,\n 'ڈھیر': 386,\n 'عمارتوں': 387,\n 'شارٹس': 388,\n 'دکھایا': 389,\n 'گلاس': 390,\n 'طور': 391,\n 'پتھر': 392,\n 'الخلا': 393,\n 'ذریعے': 394,\n 'مارنے': 395,\n 'ٹکڑے': 396,\n 'بھیڑ': 397,\n 'شخص۔': 398,\n 'بازو': 399,\n 'جن': 400,\n 'ریموٹ': 401,\n 'ویژن': 402,\n 'ڈیسک': 403,\n 'روشن': 404,\n 'کلاک': 405,\n 'ڈونٹس': 406,\n 'آنے': 407,\n 'لمبے': 408,\n 'رن': 409,\n 'چہرے': 410,\n 'ریمپ': 411,\n 'سرفنگ': 412,\n 'کرتب': 413,\n 'ڈھکے': 414,\n 'پرندے': 415,\n 'دیگر': 416,\n 'گئی': 417,\n 'بھاگ': 418,\n 'جمع': 419,\n 'کھلا': 420,\n 'سنہرے': 421,\n 'پھلوں': 422,\n 'اسے': 423,\n 'پکڑنے': 424,\n 'نشان۔': 425,\n 'لوگ۔': 426,\n 'چلنے': 427,\n 'رکھے': 428,\n 'پھینک': 429,\n 'دیکھتے': 430,\n 'جھیل': 431,\n 'ریفریجریٹر': 432,\n 'wii': 433,\n 'دکان': 434,\n 'مار': 435,\n 'برش': 436,\n 'پل': 437,\n 'کالے': 438,\n 'پھول': 439,\n 'بیڈ': 440,\n 'گٹار': 441,\n 'آلود': 442,\n 'چٹان': 443,\n 'ٹرے': 444,\n 'گوشت': 445,\n 'ٹب': 446,\n 'پنیر': 447,\n 'برتن': 448,\n 'تیاری': 449,\n 'رکھی': 450,\n 'یا': 451,\n 'بغیر': 452,\n 'دونوں': 453,\n 'دھاری': 454,\n 'کاغذ': 455,\n 'دریا': 456,\n 'اشیاء': 457,\n 'سرفر': 458,\n 'عورت۔': 459,\n 'بند': 460,\n 'بورڈنگ': 461,\n 'ساتھ۔': 462,\n 'ڈسپلے': 463,\n 'ہاتھیوں': 464,\n 'حصہ': 465,\n 'کوٹ': 466,\n 'کتاب': 467,\n 'بھیڑوں': 468,\n 'ڈیکر': 469,\n 'غسل': 470,\n 'جینز': 471,\n 'پکڑا': 472,\n 'پرانے': 473,\n 'پڑھ': 474,\n 'جیٹ': 475,\n 'گھوم': 476,\n 'ہوتی': 477,\n 'نہیں': 478,\n 'کاروں': 479,\n 'روم۔': 480,\n 'بنچ': 481,\n 'دکھائی': 482,\n 'تندور': 483,\n 'بوتل': 484,\n 'لہروں': 485,\n 'ریستوراں': 486,\n 'پولیس': 487,\n 'یونیفارم': 488,\n 'سیب': 489,\n 'شدہ': 490,\n 'کپڑے': 491,\n 'افراد': 492,\n 'ڈونٹ': 493,\n 'بیٹ': 494,\n 'شاور': 495,\n 'ٹوکری': 496,\n 'نشانات': 497,\n 'آگے': 498,\n 'بارش': 499,\n 'لطف': 500,\n 'کراس': 501,\n 'دور': 502,\n 'آلات': 503,\n 'تھا۔': 504,\n 'اندوز': 505,\n 'پلیٹ۔': 506,\n 'جوتے': 507,\n 'لونگ': 508,\n 'کاریں': 509,\n 'ٹیبل': 510,\n 'چوٹی': 511,\n 'ٹریک': 512,\n 'کھلونا': 513,\n 'ہونے': 514,\n 'جیسے': 515,\n 'تک': 516,\n 'ریکٹ': 517,\n 'لائن': 518,\n 'ریوڑ': 519,\n 'دھاتی': 520,\n 'تالاب': 521,\n 'سائیکلوں': 522,\n 'چڑیا': 523,\n 'چھتریوں': 524,\n 'پھل': 525,\n 'جانے': 526,\n 'پکڑی': 527,\n 'کھلی': 528,\n 'کمبل': 529,\n 'واک': 530,\n 'ٹرین۔': 531,\n 'پیدل': 532,\n 'اسٹینڈ': 533,\n 'چوراہے': 534,\n 'جانوروں': 535,\n 'شیلف': 536,\n 'اڑتا': 537,\n 'و': 538,\n 'زیادہ': 539,\n 'فارم': 540,\n 'سواری': 541,\n 'جھک': 542,\n 'حصے': 543,\n 'چلتے': 544,\n 'بیرونی': 545,\n 'کلوز': 546,\n 'پہن': 547,\n 'پلاسٹک': 548,\n 'پڑا': 549,\n 'ابر': 550,\n 'پین': 551,\n 'چھت': 552,\n 'گائیں': 553,\n 'چال': 554,\n 'اسکرین': 555,\n 'باکس': 556,\n 'چمکدار': 557,\n 'برفیلے': 558,\n 'مانیٹر': 559,\n 'چاندی': 560,\n 'چاقو': 561,\n 'باز': 562,\n 'کھیلتے': 563,\n 'اسٹریٹ': 564,\n 'لٹکا': 565,\n 'سیڑھیوں': 566,\n 'کود': 567,\n 'پاؤں': 568,\n 'پانچ': 569,\n 'برفیلی': 570,\n 'روٹی': 571,\n 'خانہ۔': 572,\n 'بے': 573,\n 'سبزیاں': 574,\n 'صحن': 575,\n 'بوڑھی': 576,\n 'اشارہ': 577,\n 'ٹائل': 578,\n 'کرسیاں': 579,\n 'پرانا': 580,\n 'آ': 581,\n 'فرنیچر': 582,\n 'بورڈز': 583,\n 'شامل': 584,\n 'بیٹھ': 585,\n 'چولہے': 586,\n 'تعمیراتی': 587,\n 'اسکیئر': 588,\n 'سنتری': 589,\n 'لائٹس': 590,\n 'بازار': 591,\n 'کارکن': 592,\n 'ٹیم': 593,\n 'سارے': 594,\n 'آدمی،': 595,\n 'پیزا۔': 596,\n 'کشتیاں': 597,\n 'ریس': 598,\n 'سورج': 599,\n 'لمبا': 600,\n 'قینچی': 601,\n 'ڈال': 602,\n 'چاکلیٹ': 603,\n 'چڑھ': 604,\n 'گئے': 605,\n 'پیالہ': 606,\n 'مٹی': 607,\n 'پہاڑوں': 608,\n 'پلیٹوں': 609,\n 'بھیڑیں': 610,\n 'موم': 611,\n 'اٹھا': 612,\n 'چشمہ': 613,\n 'لٹک': 614,\n 'اسٹور': 615,\n 'بار': 616,\n 'گلے': 617,\n 'عورتیں': 618,\n 'تیر': 619,\n 'سائیڈ': 620,\n 'میٹر': 621,\n 'اسٹیج': 622,\n 'ٹین': 623,\n 'سالگرہ': 624,\n 'ندی': 625,\n 'دانت': 626,\n 'پتنگیں': 627,\n 'لگتا': 628,\n 'ترتیب': 629,\n 'مائکروویو': 630,\n 'کرسیوں': 631,\n 'کمرہ۔': 632,\n 'پچ': 633,\n 'لکھا': 634,\n 'وہاں': 635,\n 'آؤٹ': 636,\n 'گچھا': 637,\n 'سرسبز': 638,\n 'فام': 639,\n 'ڈبے': 640,\n 'تھی۔': 641,\n 'پتوں': 642,\n 'اسکی': 643,\n 'وردی': 644,\n 'خلاف': 645,\n 'کتابوں': 646,\n 'اکیلا': 647,\n 'کنکریٹ': 648,\n 'ڈھکا': 649,\n 'تازہ': 650,\n 'پٹری': 651,\n 'ہیڈ': 652,\n 'پیٹھ': 653,\n 'پشت': 654,\n 'پتھروں': 655,\n 'شاخ': 656,\n 'سطح': 657,\n 'گول': 658,\n 'میں،': 659,\n 'تھے۔': 660,\n 'ماؤس': 661,\n 'کشتیوں': 662,\n 'مشروبات': 663,\n 'ڈش': 664,\n 'عوامی': 665,\n 'گروہ': 666,\n 'سائن': 667,\n 'شکل': 668,\n 'پرندوں': 669,\n 'بائیک': 670,\n 'ہاتھوں': 671,\n 'گھٹنے': 672,\n 'باسکٹ': 673,\n 'میز۔': 674,\n 'ہر': 675,\n 'کتا۔': 676,\n 'ماں': 677,\n 'گرم': 678,\n 'گاجر': 679,\n 'چلتی': 680,\n 'قالین': 681,\n 'سونے': 682,\n 'چہرہ': 683,\n 'دیواروں': 684,\n 'رکھتا': 685,\n 'کچی': 686,\n 'چشمے': 687,\n 'متعدد': 688,\n 'کیمرہ': 689,\n 'بھی': 690,\n 'کھیلنے': 691,\n 'بینڈ': 692,\n 'لگائے': 693,\n 'جاتی': 694,\n 'گیا۔': 695,\n 'نمائش': 696,\n 'داڑھی': 697,\n 'تمام': 698,\n 'خشک': 699,\n 'پہلو': 700,\n 'پہنچ': 701,\n 'چھڑی': 702,\n 'براؤن': 703,\n 'سلاد': 704,\n 'مائیکروفون': 705,\n 'سویٹر': 706,\n 'گھور': 707,\n 'اسکول': 708,\n 'ٹوتھ': 709,\n 'اچھی': 710,\n 'دھات': 711,\n 'کتوں': 712,\n 'سائز': 713,\n 'مجسمے': 714,\n 'کنٹرولر': 715,\n 'رنگوں': 716,\n 'انجن': 717,\n 'کیلا': 718,\n 'موجود': 719,\n 'آئینہ': 720,\n 'جسے': 721,\n 'کریم': 722,\n 'بعد': 723,\n 'الماریاں': 724,\n 'دستانے': 725,\n 'دیا': 726,\n 'خاندان': 727,\n 'لیٹ': 728,\n 'جدید': 729,\n 'گہرے': 730,\n 'کپڑوں': 731,\n 'آتا': 732,\n 'ٹکڑوں': 733,\n 'پودوں': 734,\n 'بس۔': 735,\n 'جہاں': 736,\n 'لینے': 737,\n 'دیتی': 738,\n 'سخت': 739,\n 'گرے': 740,\n 'تصاویر': 741,\n 'برتنوں': 742,\n 'تھیلے': 743,\n 'کنٹرول': 744,\n 'ہوٹل': 745,\n 'مچھلی': 746,\n 'گھڑی۔': 747,\n 'سٹی': 748,\n 'میزوں': 749,\n 'کھڑکیوں': 750,\n 'مکمل': 751,\n 'امریکی': 752,\n 'شادی': 753,\n 'گودی': 754,\n 'تکیے': 755,\n 'کھاتے': 756,\n 'ٹینک': 757,\n 'مدد': 758,\n 'غیر': 759,\n 'گایوں': 760,\n 'میچ': 761,\n 'پکا': 762,\n 'نکل': 763,\n 'گیئر': 764,\n 'آگ': 765,\n 'چولہا': 766,\n 'جھاڑیوں': 767,\n 'لگی': 768,\n 'سوئنگ': 769,\n 'فرائز': 770,\n 'زرافوں': 771,\n 'بسیں': 772,\n 'پتلون': 773,\n 'لڑکوں': 774,\n 'مشین': 775,\n 'درمیانی': 776,\n 'خود': 777,\n 'ہنس': 778,\n 'مجسمہ': 779,\n 'فوجی': 780,\n 'دائیں': 781,\n 'کٹے': 782,\n 'غروب': 783,\n 'پول': 784,\n 'قلم': 785,\n 'تقریب': 786,\n 'گرافٹی': 787,\n 'شاداب': 788,\n 'دیکھنے': 789,\n 'بیر': 790,\n 'پرفارم': 791,\n 'پوسٹ': 792,\n 'بائیں': 793,\n 'اپ۔': 794,\n 'تار': 795,\n 'گھرا': 796,\n 'بنیان': 797,\n 'لات': 798,\n 'والوں': 799,\n 'گرل': 800,\n 'مل': 801,\n 'سیمنٹ': 802,\n 'موسم': 803,\n 'پھینکنے': 804,\n 'طیارے': 805,\n 'دفتر': 806,\n 'تیز': 807,\n 'چاروں': 808,\n 'لگے': 809,\n 'نوجوانوں': 810,\n 'گاڑیوں': 811,\n 'دروازہ': 812,\n 'آئس': 813,\n 'سنک،': 814,\n 'ریستوران': 815,\n 'لیمپ': 816,\n 'جاتے': 817,\n 'چینی': 818,\n 'گا': 819,\n 'بچھا': 820,\n 'لان': 821,\n 'ہائیڈرنٹ۔': 822,\n 'پڑی': 823,\n 'رقص': 824,\n 'کانٹے': 825,\n 'رسی': 826,\n 'ہو۔': 827,\n 'باتیں': 828,\n 'آرائشی': 829,\n 'میں۔': 830,\n 'صوفہ': 831,\n 'اسنو': 832,\n 'پودے': 833,\n 'منسلک': 834,\n 'دانتوں': 835,\n 'اڑاتے': 836,\n 'بوڑھے': 837,\n 'بلیاں': 838,\n 'آرٹ': 839,\n 'سجا': 840,\n 'پیشہ': 841,\n 'بیک': 842,\n 'پگڈنڈی': 843,\n 'ریک': 844,\n 'کٹنگ': 845,\n 'کرسمس': 846,\n 'گود': 847,\n 'مسافروں': 848,\n 'فلیٹ': 849,\n 'مخالف': 850,\n 'لٹکی': 851,\n 'سگریٹ': 852,\n 'اسکرٹ': 853,\n 'فروخت': 854,\n 'پک': 855,\n 'ٹماٹر': 856,\n 'ابھی': 857,\n 'بچی': 858,\n 'ریتیلے': 859,\n 'آدھا': 860,\n 'ستھرا': 861,\n 'جہاز۔': 862,\n 'مشق': 863,\n 'پوش': 864,\n 'چرچ': 865,\n 'ہلکے': 866,\n 'وین': 867,\n 'آس': 868,\n 'پیلا': 869,\n 'گیلے': 870,\n 'تیراکی': 871,\n 'عجیب': 872,\n 'ٹاپنگز': 873,\n 'الخلاء': 874,\n 'چراگاہ': 875,\n 'ٹوائلٹ۔': 876,\n 'دیتا': 877,\n 'فریج': 878,\n 'کھلونے': 879,\n 'نکال': 880,\n 'کرب': 881,\n 'مسکراتا': 882,\n 'جوڑا۔': 883,\n 'آلو': 884,\n 'وائی': 885,\n 'گندے': 886,\n 'فاصلے': 887,\n 'پیسٹری': 888,\n 'عمر': 889,\n 'آدھے': 890,\n 'پڑے': 891,\n 'نمبر': 892,\n 'جھولے': 893,\n 'دھکیل': 894,\n 'کم': 895,\n 'بیل': 896,\n 'پٹی': 897,\n 'پیچھا': 898,\n 'بیڈروم': 899,\n 'پہلے': 900,\n 'اسکیئنگ': 901,\n 'ڈور': 902,\n 'خوش': 903,\n 'کاٹنے': 904,\n 'آتی': 905,\n 'جیسا': 906,\n 'پلیٹیں': 907,\n 'علاقہ': 908,\n 'ایسا': 909,\n 'مین': 910,\n 'پردے': 911,\n 'قطبی': 912,\n 'ایریا': 913,\n 'چمڑے': 914,\n 'بیٹھتے': 915,\n 'تھامے': 916,\n 'کیچر': 917,\n 'غول': 918,\n 'باغ': 919,\n 'مسکراتی': 920,\n 'بنایا': 921,\n 'طرز': 922,\n 'تنگ': 923,\n 'تصویریں': 924,\n 'سیڑھی': 925,\n 'پارٹی': 926,\n 'ریلنگ': 927,\n 'پر۔': 928,\n 'وسط': 929,\n 'گلدستے': 930,\n 'چٹنی': 931,\n 'دوست': 932,\n 'بچہ۔': 933,\n 'آف': 934,\n 'چھ': 935,\n 'آفتاب': 936,\n 'پینٹنگ': 937,\n 'اسکارف': 938,\n 'چمچ': 939,\n 'سجاوٹ': 940,\n 'چمنی': 941,\n 'کیے': 942,\n 'ہوئے۔': 943,\n 'عبور': 944,\n 'آنکھوں': 945,\n 'جزوی': 946,\n 'پیڈل': 947,\n '2': 948,\n 'شہری': 949,\n 'ور': 950,\n 'وہیل': 951,\n 'گھڑا': 952,\n 'پکنک': 953,\n 'کھلاڑیوں': 954,\n 'برنگی': 955,\n 'پینے': 956,\n 'عینک': 957,\n 'صرف': 958,\n 'سوئمنگ': 959,\n 'پیپر': 960,\n 'اسکیئرز': 961,\n 'عورت،': 962,\n 'گندا': 963,\n 'کنٹری': 964,\n 'اونچا': 965,\n 'چیک': 966,\n 'مشروب': 967,\n 'فوڈ': 968,\n 'گھری': 969,\n 'سائیکلیں': 970,\n 'لڑ': 971,\n 'پریڈ': 972,\n 'چٹانوں': 973,\n 'حفاظتی': 974,\n 'افریقی': 975,\n 'الماری': 976,\n 'موڑ': 977,\n 'بلیک': 978,\n 'پہنے،': 979,\n 'پیالہ۔': 980,\n 'جھنڈ': 981,\n 'شاٹ': 982,\n 'شو': 983,\n 'مسکراتے': 984,\n 'جوتوں': 985,\n 'رکھ': 986,\n 'واٹر': 987,\n 'ترتیبی': 988,\n 'ہوئے،': 989,\n 'سویٹ': 990,\n 'لڑکیوں': 991,\n 'پائپ': 992,\n 'موسیقی': 993,\n 'ٹانگوں': 994,\n 'کندھے': 995,\n 'نیلا': 996,\n 'ٹوپیاں': 997,\n 'پرس': 998,\n 'سایہ': 999,\n 'رکاوٹ': 1000,\n ...}"},"metadata":{}}]},{"cell_type":"markdown","source":"This is dictionary containing urdu words as keys and their corresponding indices as values. It represents the vocabulary learned by the tokenizer during the fitting process","metadata":{}},{"cell_type":"code","source":"# get maximum length of the caption available\nmax_length = max(len(ucap.split()) for ucap in data['urdu_caption'])\nmax_length","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:50:59.002101Z","iopub.execute_input":"2024-06-05T06:50:59.002429Z","iopub.status.idle":"2024-06-05T06:50:59.827377Z","shell.execute_reply.started":"2024-06-05T06:50:59.002404Z","shell.execute_reply":"2024-06-05T06:50:59.826458Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"94"},"metadata":{}}]},{"cell_type":"markdown","source":"## Splitting the Data","metadata":{}},{"cell_type":"markdown","source":"5 Captions per image\n* Train = 70% (562089)\n* Validation = 20% (140523)\n* Test = 10% (78069)\n\n1 caption per image:\n* Train = 72% (112370)\n* Validation = 18% (28093)\n* Test = 10% (15607)","metadata":{}},{"cell_type":"code","source":"X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(data['image'], data['urdu_caption'], \n                                                                test_size = 0.10, random_state = 30, shuffle = True)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_train_valid, Y_train_valid, \n                                                      test_size = 0.2, random_state = 30, shuffle = True)\n\nprint('Train set size:', X_train.shape)\nprint('Validation set size:', X_valid.shape)\nprint('Test set size:', X_test.shape)\nprint('Total data size', X_train.shape[0] + X_valid.shape[0] + X_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:50:59.828420Z","iopub.execute_input":"2024-06-05T06:50:59.828679Z","iopub.status.idle":"2024-06-05T06:51:00.076090Z","shell.execute_reply.started":"2024-06-05T06:50:59.828655Z","shell.execute_reply":"2024-06-05T06:51:00.075044Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Train set size: (449481,)\nValidation set size: (112371,)\nTest set size: (62428,)\nTotal data size 624280\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"markdown","source":"* A model architecture that will combine visual information from images with textual information from partial captions to generate captions for images.\n* The model will learn to generate captions based on both the visual content and the context provided by the captions.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # image feature extractor model\n    inputs1 = Input(shape = (1000,))             # defines an input layer for the image features.\n    fe1 = BatchNormalization()(inputs1)          # applies batch normalization to the input image features.\n    fe2 = Dense(512, activation = 'relu')(fe1)   # applies a dense layer with 512 units and ReLU activation to the batch-normalized image features.\n\n    # partial caption sequence model\n    inputs2 = Input(shape = (max_length,))       # defines an input layer for the partial caption sequences.\n    se1 = Embedding(vocab_size, 512)(inputs2)   # embeds the input sequences into dense vectors of size 512. \n                                                 # this layer uses an embedding matrix with a vocabulary size of vocab_size.\n    se2 = BatchNormalization()(se1)              # applies batch normalization to the embedded sequences.\n    se3 = LSTM(256)(se2)                         # applies a LSTM layer with 256 units to the batch-normalized embedded sequences.\n\n    # decoder model\n    decoder = Concatenate()([fe2, se3])                  # concatenates output features from the image extractor and partial caption sequence models.\n    decoder2 = Dense(512, activation = 'relu')(decoder)  # applies a dense layer with 512 units and ReLU activation to the concatenated features.\n    outputs = Dense(vocab_size,                          # applies a dense layer with vocab_size units and softmax activation to produce the output\n                    activation = 'softmax')(decoder2)    # probability distribution over the vocabulary.\n\n    # merge 2 networks\n    model = Model(inputs = [inputs1, inputs2], outputs = outputs)\n\n    optimizer = Adam(learning_rate = 0.005,\n                     clipvalue = 5.0)          # technique used to limit the magnitude of gradients during training.\n                                               # helps stabilize the training process, especially with exploding gradients.\n\n    model.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:04:18.393808Z","iopub.execute_input":"2024-06-02T15:04:18.394101Z","iopub.status.idle":"2024-06-02T15:04:19.122226Z","shell.execute_reply.started":"2024-06-02T15:04:18.394076Z","shell.execute_reply":"2024-06-02T15:04:19.121470Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The merged model combines the image feature extractor model and the partial caption sequence model into a single model.<br>\nIt takes both the image features (inputs1) and the partial caption sequences (inputs2) as inputs and produces the output probabilities over the vocabulary (outputs).","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:04:19.123334Z","iopub.execute_input":"2024-06-02T15:04:19.123673Z","iopub.status.idle":"2024-06-02T15:04:19.152624Z","shell.execute_reply.started":"2024-06-02T15:04:19.123636Z","shell.execute_reply":"2024-06-02T15:04:19.151789Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │ \u001b[38;5;34m13,916,160\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │      \u001b[38;5;34m4,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m2,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m512,512\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m787,456\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m393,728\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27180\u001b[0m)     │ \u001b[38;5;34m13,943,340\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,916,160</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">512,512</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27180</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,943,340</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,559,244\u001b[0m (112.76 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,559,244</span> (112.76 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,556,220\u001b[0m (112.75 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,556,220</span> (112.75 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,024\u001b[0m (11.81 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,024</span> (11.81 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:04:19.153606Z","iopub.execute_input":"2024-06-02T15:04:19.153848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# create data generator to get data in batch (avoids session crash)\ndef data_generator(data, features, tokenizer, max_length, vocab_size, batch_size):\n    # loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data['image'].tolist():\n            n += 1\n            captions = data[data['image'] == key]['urdu_caption']   # ek img k sary captions, 'captions' is a list\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pairs\n                    in_seq, out_seq = seq[:i], seq[i]    # for predicting next word for a given word\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]   # to make length of all captions same by appending zeros\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]   # to tell the index where the word is stored in tokenizer\n                    \n                    # store the sequences (all these are 2D lists now)\n                    X1.append(features[key.split('.')[0]][0].cpu().numpy())\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield (X1, X2), y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.idle":"2024-06-02T15:04:19.561410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = {'image': X_train.tolist(), 'urdu_caption': Y_train.tolist()}\ntrain_set = pd.DataFrame(train_set)\ntrain_set","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:04:19.562531Z","iopub.execute_input":"2024-06-02T15:04:19.562958Z","iopub.status.idle":"2024-06-02T15:04:19.718517Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                image                                       urdu_caption\n0       img112934.jpg  endseq گائیوں کا ایک غول خاردار باڑ کے پاس کھڑ...\n1       img062210.jpg  endseq یہ ایک پل کے بارے میں انتباہ کے نشان کی...\n2       img015619.jpg  endseq لوگ میز کے پاس بیٹھے مسکرا رہے ہیں اور ...\n3       img060858.jpg  endseq پٹریوں پر نارنجی پٹی والی سفید ٹرین sta...\n4       img128054.jpg  endseq پارک میں ایک سیاہ اور سفید کتا برف میں ...\n...               ...                                                ...\n337106  img070574.jpg  endseq مردوں کا ایک گروپ تلوار سے سفید چادر کا...\n337107  img046623.jpg  endseq ایک آدمی لمبی بازو کی قمیض پہنے سمارٹ ف...\n337108  img007919.jpg  endseq لوگوں کا ایک گروپ زرافے کے گرد کھڑا ہے۔...\n337109  img059922.jpg  endseq ایک میز کے اوپر چمکدار ڈونٹس سے بھرے تی...\n337110  img071134.jpg  endseq یہ ماں خوش ہے کہ اس کے بیٹے اور بیٹی کو...\n\n[337111 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>urdu_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img112934.jpg</td>\n      <td>endseq گائیوں کا ایک غول خاردار باڑ کے پاس کھڑ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img062210.jpg</td>\n      <td>endseq یہ ایک پل کے بارے میں انتباہ کے نشان کی...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img015619.jpg</td>\n      <td>endseq لوگ میز کے پاس بیٹھے مسکرا رہے ہیں اور ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img060858.jpg</td>\n      <td>endseq پٹریوں پر نارنجی پٹی والی سفید ٹرین sta...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img128054.jpg</td>\n      <td>endseq پارک میں ایک سیاہ اور سفید کتا برف میں ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>337106</th>\n      <td>img070574.jpg</td>\n      <td>endseq مردوں کا ایک گروپ تلوار سے سفید چادر کا...</td>\n    </tr>\n    <tr>\n      <th>337107</th>\n      <td>img046623.jpg</td>\n      <td>endseq ایک آدمی لمبی بازو کی قمیض پہنے سمارٹ ف...</td>\n    </tr>\n    <tr>\n      <th>337108</th>\n      <td>img007919.jpg</td>\n      <td>endseq لوگوں کا ایک گروپ زرافے کے گرد کھڑا ہے۔...</td>\n    </tr>\n    <tr>\n      <th>337109</th>\n      <td>img059922.jpg</td>\n      <td>endseq ایک میز کے اوپر چمکدار ڈونٹس سے بھرے تی...</td>\n    </tr>\n    <tr>\n      <th>337110</th>\n      <td>img071134.jpg</td>\n      <td>endseq یہ ماں خوش ہے کہ اس کے بیٹے اور بیٹی کو...</td>\n    </tr>\n  </tbody>\n</table>\n<p>337111 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 100\nsteps = len(X_train) // batch_size\n\n#Define a ModelCheckpoint callback\n#Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = '/kaggle/working/model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n    \n    # fit for one epoch\n    model.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:06:06.801469Z","iopub.execute_input":"2024-05-31T22:06:06.801807Z","iopub.status.idle":"2024-06-01T04:45:27.932354Z","shell.execute_reply.started":"2024-05-31T22:06:06.801782Z","shell.execute_reply":"2024-06-01T04:45:27.931077Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[1m3371/3371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23948s\u001b[0m 7s/step - accuracy: 0.6072 - loss: 7.9716\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('Trained_YOLO_LSTM_3c1e.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T04:45:27.934729Z","iopub.execute_input":"2024-06-01T04:45:27.935018Z","iopub.status.idle":"2024-06-01T04:45:28.397206Z","shell.execute_reply.started":"2024-06-01T04:45:27.934993Z","shell.execute_reply":"2024-06-01T04:45:28.396327Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:47:49.668692Z","iopub.execute_input":"2024-06-05T06:47:49.669039Z","iopub.status.idle":"2024-06-05T06:47:49.673020Z","shell.execute_reply.started":"2024-06-05T06:47:49.669005Z","shell.execute_reply":"2024-06-05T06:47:49.672042Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model3c1e = load_model('/kaggle/input/yolo-gru-4c1e/Trained_YOLO_GRU_4c1e.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:51:00.077198Z","iopub.execute_input":"2024-06-05T06:51:00.077490Z","iopub.status.idle":"2024-06-05T06:51:01.544637Z","shell.execute_reply.started":"2024-06-05T06:51:00.077465Z","shell.execute_reply":"2024-06-05T06:51:01.543694Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"optimizer1 = Adam(learning_rate = 0.005,\n                     clipvalue = 5.0)\nmodel3c1e.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer1,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:05:05.150781Z","iopub.execute_input":"2024-06-02T15:05:05.151538Z","iopub.status.idle":"2024-06-02T15:05:05.159657Z","shell.execute_reply.started":"2024-06-02T15:05:05.151508Z","shell.execute_reply":"2024-06-02T15:05:05.158814Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model3c1e.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:05:05.478164Z","iopub.execute_input":"2024-06-02T15:05:05.478988Z","iopub.status.idle":"2024-06-02T15:05:05.505282Z","shell.execute_reply.started":"2024-06-02T15:05:05.478957Z","shell.execute_reply":"2024-06-02T15:05:05.504461Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │ \u001b[38;5;34m13,916,160\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │      \u001b[38;5;34m4,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m2,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m512,512\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m787,456\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m393,728\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27180\u001b[0m)     │ \u001b[38;5;34m13,943,340\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,916,160</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">512,512</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27180</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13,943,340</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,559,244\u001b[0m (112.76 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,559,244</span> (112.76 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,556,220\u001b[0m (112.75 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,556,220</span> (112.75 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,024\u001b[0m (11.81 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,024</span> (11.81 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# with strategy.scope():\n# train the model\nepochs = 1\nbatch_size = 100\nsteps = len(X_train) // batch_size\n\n#Define a ModelCheckpoint callback\n#Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = '/kaggle/working/model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n#     monitor = 'val_loss/\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n\n    # fit for one epoch\n    model3c1e.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:05:13.105904Z","iopub.execute_input":"2024-06-02T15:05:13.106524Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[1m2319/3371\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2:06:41\u001b[0m 7s/step - accuracy: 0.3874 - loss: 3.2326","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generate a Caption using the Model","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word,index, in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:47:51.322613Z","iopub.execute_input":"2024-06-05T06:47:51.323049Z","iopub.status.idle":"2024-06-05T06:47:51.328071Z","shell.execute_reply.started":"2024-06-05T06:47:51.323017Z","shell.execute_reply":"2024-06-05T06:47:51.327180Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# generate caption for an image\ndef predict_caption(model, image, tokenizer, max_length):\n    # add start tag for generation process\n    in_text = 'endseq'\n    # iterate over the max length of sequence\n    for i in range(max_length):\n        # encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        \n        # pad the sequence\n        sequence = pad_sequences([sequence], max_length)\n        \n        # predict next word\n        yhat = model.predict([image, sequence], verbose=0)\n        \n        # get index with high probability\n        yhat = np.argmax(yhat)\n        \n        # convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        \n        # stop if word not found\n        if word is None:\n            break\n        # append word as input for generating next word\n        in_text += \" \" + word\n        \n        # stop if we reach end tag\n        if word == 'startseq':\n            break\n    return in_text","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:47:51.329412Z","iopub.execute_input":"2024-06-05T06:47:51.329736Z","iopub.status.idle":"2024-06-05T06:47:51.337233Z","shell.execute_reply.started":"2024-06-05T06:47:51.329707Z","shell.execute_reply":"2024-06-05T06:47:51.336424Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# validate with test data\nactual, predicted = list(), list()\n\nfor key in tqdm(X_test.iloc[:5]):\n    # get actual caption\n    captions = data[data['image'] == key]['urdu_caption']\n    \n    # predict the caption for image\n    y_pred = predict_caption(model3c1e, features[key.split('.')[0]], tokenizer, max_length) \n    \n    # split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    \n    # append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n    \n# calcuate BLEU score\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(0.1, 0.3, 0.6)))\nprint(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"execution":{"iopub.status.busy":"2024-06-05T07:33:03.357473Z","iopub.execute_input":"2024-06-05T07:33:03.358008Z","iopub.status.idle":"2024-06-05T07:33:08.018403Z","shell.execute_reply.started":"2024-06-05T07:33:03.357958Z","shell.execute_reply":"2024-06-05T07:33:08.017385Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56854a16b16c4659ad1c300c44264467"}},"metadata":{}},{"name":"stdout","text":"BLEU-1: 0.124538\nBLEU-2: 0.386588\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_caption(image_name):\n    # load the image\n    image_id = image_name.split('.')[0]\n    img_path = \"/kaggle/input/images/Images/\" + image_name\n    image = Image.open(img_path)\n\n    # Predict caption for image\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)[8:][:-6]\n    print('--------------------Predicted--------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T01:25:05.773484Z","iopub.execute_input":"2024-03-23T01:25:05.774329Z","iopub.status.idle":"2024-03-23T01:25:05.779666Z","shell.execute_reply.started":"2024-03-23T01:25:05.774293Z","shell.execute_reply":"2024-03-23T01:25:05.778651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"img000000.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-03-23T01:25:06.846516Z","iopub.execute_input":"2024-03-23T01:25:06.847193Z","iopub.status.idle":"2024-03-23T01:25:07.386353Z","shell.execute_reply.started":"2024-03-23T01:25:06.84714Z","shell.execute_reply":"2024-03-23T01:25:07.385128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}