{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7980273,"sourceType":"datasetVersion","datasetId":4696823},{"sourceId":8564412,"sourceType":"datasetVersion","datasetId":5120042},{"sourceId":8952731,"sourceType":"datasetVersion","datasetId":5387903},{"sourceId":8952768,"sourceType":"datasetVersion","datasetId":5387931},{"sourceId":8976847,"sourceType":"datasetVersion","datasetId":5405143},{"sourceId":9006178,"sourceType":"datasetVersion","datasetId":5425653}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom textwrap import wrap\n\nimport os\nimport gc       # garbage collection: for memory allocation and deallocation\nimport pickle\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications import VGG16\nimport tensorflow as tf\n\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import to_categorical, plot_model, Sequence\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding, Dropout, add, BatchNormalization, Concatenate\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# from nltk.translate.bleu_score import corpus_bleu\n\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:40:57.313419Z","iopub.execute_input":"2024-07-22T00:40:57.313771Z","iopub.status.idle":"2024-07-22T00:40:57.407250Z","shell.execute_reply.started":"2024-07-22T00:40:57.313744Z","shell.execute_reply":"2024-07-22T00:40:57.406399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"BLEU score implementation.\"\"\"\nimport math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\n\nfrom nltk.util import ngrams\n\n\nclass Fraction(_Fraction):\n    \"\"\"Fraction with _normalize=False support for 3.12\"\"\"\n\n    def __new__(cls, numerator=0, denominator=None, _normalize=False):\n        if sys.version_info >= (3, 12):\n            self = super().__new__(cls, numerator, denominator)\n        else:\n            self = super().__new__(cls, numerator, denominator, _normalize=_normalize)\n        self._normalize = _normalize\n        self._original_numerator = numerator\n        self._original_denominator = denominator\n        return self\n\n    @property\n    def numerator(self):\n        if not self._normalize:\n            return self._original_numerator\n        return super().numerator\n\n    @property\n    def denominator(self):\n        if not self._normalize:\n            return self._original_denominator\n        return super().denominator\n\ndef sentence_bleu(\n    references,\n    hypothesis,\n    weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None,\n    auto_reweigh=False,):\n    return corpus_bleu([references], [hypothesis], weights, smoothing_function, auto_reweigh)\n\ndef corpus_bleu(\n    list_of_references,\n    hypotheses,\n    weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None,\n    auto_reweigh=False,):\n    # Before proceeding to compute BLEU, perform sanity checks.\n\n    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n    hyp_lengths, ref_lengths = 0, 0\n\n    assert len(list_of_references) == len(hypotheses), ()\n\n    try:\n        weights[0][0]\n    except:\n        weights = [weights]\n    max_weight_length = max(len(weight) for weight in weights)\n\n    # Iterate through each hypothesis and their corresponding references.\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        # For each order of ngram, calculate the numerator and\n        # denominator for the corpus-level modified precision.\n        for i in range(1, max_weight_length + 1):\n            p_i = modified_precision(references, hypothesis, i)\n            p_numerators[i] += p_i.numerator\n            p_denominators[i] += p_i.denominator\n\n        # Calculate the hypothesis length and the closest reference length.\n        # Adds them to the corpus-level hypothesis and reference counts.\n        hyp_len = len(hypothesis)\n        hyp_lengths += hyp_len\n        ref_lengths += closest_ref_length(references, hyp_len)\n\n    # Calculate corpus-level brevity penalty.\n    bp = brevity_penalty(ref_lengths, hyp_lengths)\n\n    # Collects the various precision values for the different ngram orders.\n    p_n = [Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n        for i in range(1, max_weight_length + 1)]\n\n    # Returns 0 if there's no matching n-grams\n    # We only need to check for p_numerators[1] == 0, since if there's\n    # no unigrams, there won't be any higher order ngrams.\n    if p_numerators[1] == 0:\n        return 0 if len(weights) == 1 else [0] * len(weights)\n\n    # If there's no smoothing, set use method0 from SmoothinFunction class.\n    if not smoothing_function:\n        smoothing_function = SmoothingFunction().method0\n    # Smoothen the modified precision.\n    # Note: smoothing_function() may convert values into floats;\n    #       it tries to retain the Fraction object as much as the\n    #       smoothing method allows.\n    p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths)\n\n    bleu_scores = []\n    for weight in weights:\n        # Uniformly re-weighting based on maximum hypothesis lengths if largest\n        # order of n-grams < 4 and weights is set at default.\n        if auto_reweigh:\n            if hyp_lengths < 4 and weight == (0.25, 0.25, 0.25, 0.25):\n                weight = (1 / hyp_lengths,) * hyp_lengths\n\n        s = (w_i * math.log(p_i) for w_i, p_i in zip(weight, p_n) if p_i > 0)\n        s = bp * math.exp(math.fsum(s))\n        bleu_scores.append(s)\n    return bleu_scores[0] if len(weights) == 1 else bleu_scores\n\n\ndef modified_precision(references, hypothesis, n):\n    # Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (Counter(ngrams(reference, n)) if len(reference) >= n else Counter())\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()}\n\n    numerator = sum(clipped_counts.values())\n    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n    # Usually this happens when the ngram order is > len(reference).\n    denominator = max(1, sum(counts.values()))\n\n    return Fraction(numerator, denominator, _normalize=False)\n\n\ndef closest_ref_length(references, hyp_len):\n    ref_lens = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))\n    return closest_ref_len\n\n\ndef brevity_penalty(closest_ref_len, hyp_len):\n    if hyp_len > closest_ref_len:\n        return 1\n    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n    elif hyp_len == 0:\n        return 0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)\n\n\nclass SmoothingFunction:\n    def __init__(self, epsilon=0.1, alpha=5, k=5):\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.k = k\n\n    def method0(self, p_n, *args, **kwargs):\n        \"\"\"\n        No smoothing.\n        \"\"\"\n        p_n_new = []\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator != 0:\n                p_n_new.append(p_i)\n            else:\n                _msg = str().format(i + 1)\n                warnings.warn(_msg)\n                # When numerator==0 where denonminator==0 or !=0, the result\n                # for the precision score should be equal to 0 or undefined.\n                # Due to BLEU geometric mean computation in logarithm space,\n                # we we need to take the return sys.float_info.min such that\n                # math.log(sys.float_info.min) returns a 0 precision score.\n                p_n_new.append(sys.float_info.min)\n        return p_n_new\n\n    def method1(self, p_n, *args, **kwargs):\n        \"\"\"\n        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n        \"\"\"\n        return [(p_i.numerator + self.epsilon) / p_i.denominator\n            if p_i.numerator == 0\n            else p_i\n            for p_i in p_n]\n\n    def method2(self, p_n, *args, **kwargs):\n        return [Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n            if i != 0\n            else p_n[0]\n            for i in range(len(p_n))]\n\n    def method3(self, p_n, *args, **kwargs):\n        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0:\n                p_n[i] = 1 / (2**incvnt * p_i.denominator)\n                incvnt += 1\n        return p_n\n\n    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        incvnt = 1\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0 and hyp_len > 1:\n                # incvnt = i + 1 * self.k / math.log(\n                #     hyp_len\n                # )  # Note that this K is different from the K from NIST.\n                # p_n[i] = incvnt / p_i.denominator\\\n                numerator = 1 / (2**incvnt * self.k / math.log(hyp_len))\n                p_n[i] = numerator / p_i.denominator\n                incvnt += 1\n        return p_n\n\n    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        m = {}\n        # Requires an precision value for an addition ngram order.\n        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n        m[-1] = p_n[0] + 1\n        for i, p_i in enumerate(p_n):\n            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n            m[i] = p_n[i]\n        return p_n\n\n    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        # This smoothing only works when p_1 and p_2 is non-zero.\n        # Raise an error with an appropriate message when the input is too short\n        # to use this smoothing technique.\n        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n        for i, p_i in enumerate(p_n):\n            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n                continue\n            else:\n                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n                # No. of ngrams in translation that matches the reference.\n                m = p_i.numerator\n                # No. of ngrams in translation.\n                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n                # Calculates the interpolated precision.\n                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n        return p_n\n\n    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        \"\"\"\n        Smoothing method 7:\n        Interpolates methods 4 and 5.\n        \"\"\"\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n        return p_n","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:40:57.822843Z","iopub.execute_input":"2024-07-22T00:40:57.823226Z","iopub.status.idle":"2024-07-22T00:40:59.113287Z","shell.execute_reply.started":"2024-07-22T00:40:57.823183Z","shell.execute_reply":"2024-07-22T00:40:59.112200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Data ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataset3cap/FINAL_COMBINED_DATASET_3CAP.csv')\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:40:59.115169Z","iopub.execute_input":"2024-07-22T00:40:59.115652Z","iopub.status.idle":"2024-07-22T00:41:02.290711Z","shell.execute_reply.started":"2024-07-22T00:40:59.115617Z","shell.execute_reply":"2024-07-22T00:41:02.289703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display Images with Captions","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# def readImage(path, img_size = 224):\n#     img = load_img(path, color_mode = 'rgb', target_size = (img_size, img_size))\n#     img = img_to_array(img)\n#     img = img/255.\n#     return img\n\n# # THIS IS NOT NECESSARY, URDU CAPTIONS THEK DISPLAY NH HO RHE\n# def display_images(temp_df):   \n#     temp_df = temp_df.reset_index(drop = True)\n#     plt.figure(figsize = (20, 20))\n#     n = 0\n#     for i in range(156070 * 5):  \n#         n += 1\n#         plt.subplot(156070, 5, n)  \n#         plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n\n#         image = readImage(f\"/kaggle/input/images/Images/{temp_df.image[i]}\")\n#         plt.imshow(image)\n#         plt.title(\"\\n\".join(wrap(temp_df.urdu_caption[i][-1::-1], 20)))\n#         plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:53:27.08083Z","iopub.execute_input":"2024-04-12T08:53:27.081216Z","iopub.status.idle":"2024-04-12T08:53:27.086265Z","shell.execute_reply.started":"2024-04-12T08:53:27.081184Z","shell.execute_reply":"2024-04-12T08:53:27.085418Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display_images(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T08:53:27.088163Z","iopub.execute_input":"2024-04-12T08:53:27.088438Z","iopub.status.idle":"2024-04-12T08:53:27.101693Z","shell.execute_reply.started":"2024-04-12T08:53:27.088411Z","shell.execute_reply":"2024-04-12T08:53:27.101019Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model to Extract Features","metadata":{}},{"cell_type":"code","source":"# Load the Model\ncnn = YOLO(\"yolov8m-cls.pt\")","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Image Features","metadata":{}},{"cell_type":"code","source":"# features = {}\n# directory = '/kaggle/input/images/Images'\n\n# for img_name in tqdm(os.listdir(directory)): \n    \n#     # load the image from file\n#     img_path = directory + '/' + img_name\n#     image = load_img(img_path, target_size = (299, 299))\n#     image = img_to_array(image)                # convert image pixels to numpy array\n    \n#     # reshape data for model\n#     image = image.reshape((1, image.shape[0],  # width \n#                            image.shape[1],     # height\n#                            image.shape[2]      # channels (3 because of rgb)\n#                           ))\n\n#     image = preprocess_input(image)            # preprocess image for Xception\n#     feature = cnn.predict(image, verbose=0)    # extract features\n#     image_id = img_name.split('.')[0]          # get image ID\n#     features[image_id] = feature[0]            # store feature  (size = 2048)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # store features in pickle\n# pickle.dump(features, open('/kaggle/working/ImgFeaturesVGG16.pkl', 'wb'))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(features, open('ImgFeaturesVGG16Reshaped.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:25:22.870775Z","iopub.execute_input":"2024-05-05T16:25:22.871516Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load features from pickle\nwith open('/kaggle/input/yolopkl/ImgFeaturesYOLO.pkl', 'rb') as f:    \n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:41:04.299348Z","iopub.execute_input":"2024-07-22T00:41:04.299709Z","iopub.status.idle":"2024-07-22T00:41:44.041771Z","shell.execute_reply.started":"2024-07-22T00:41:04.299679Z","shell.execute_reply":"2024-07-22T00:41:44.040875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess captions","metadata":{}},{"cell_type":"code","source":"def preprocessCaption(df):\n    df['urdu_caption'] = 'endseq ' + df['urdu_caption'] + ' startseq'\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:41:44.052504Z","iopub.execute_input":"2024-07-22T00:41:44.053151Z","iopub.status.idle":"2024-07-22T00:41:44.057864Z","shell.execute_reply.started":"2024-07-22T00:41:44.053122Z","shell.execute_reply":"2024-07-22T00:41:44.056880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.apply(preprocessCaption, axis = 1)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:41:44.058985Z","iopub.execute_input":"2024-07-22T00:41:44.059285Z","iopub.status.idle":"2024-07-22T00:42:22.098145Z","shell.execute_reply.started":"2024-07-22T00:41:44.059261Z","shell.execute_reply":"2024-07-22T00:42:22.097143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[87714, 2]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:22.101117Z","iopub.execute_input":"2024-07-22T00:42:22.101825Z","iopub.status.idle":"2024-07-22T00:42:22.107937Z","shell.execute_reply.started":"2024-07-22T00:42:22.101788Z","shell.execute_reply":"2024-07-22T00:42:22.106965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the Text","metadata":{}},{"cell_type":"markdown","source":"In Python, indices typically start from 0. However, when working with tokenization in NLP, we often reserve index 0 for special tokens, such as padding tokens or unknown tokens.\n* **Padding Token:** In many NLP tasks, sequences of words or tokens are padded to ensure uniform length. Padding tokens are used to fill in the extra spaces in sequences to make them uniform. Index 0 is usually reserved for the padding token.\n<br><br>\n* **Unknown Token:** This token is used to represent words that are not present in the vocabulary. When a word that is not in the vocabulary is encountered during tokenization, it is replaced by the unknown token. Again, index 0 is often reserved for this purpose.","metadata":{}},{"cell_type":"code","source":"# tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data[\"urdu_caption\"])\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:22.109125Z","iopub.execute_input":"2024-07-22T00:42:22.109410Z","iopub.status.idle":"2024-07-22T00:42:34.833951Z","shell.execute_reply.started":"2024-07-22T00:42:22.109386Z","shell.execute_reply":"2024-07-22T00:42:34.833025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.word_index","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-22T00:42:34.835317Z","iopub.execute_input":"2024-07-22T00:42:34.835736Z","iopub.status.idle":"2024-07-22T00:42:34.874658Z","shell.execute_reply.started":"2024-07-22T00:42:34.835702Z","shell.execute_reply":"2024-07-22T00:42:34.873773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is dictionary containing urdu words as keys and their corresponding indices as values. It represents the vocabulary learned by the tokenizer during the fitting process","metadata":{}},{"cell_type":"code","source":"# get maximum length of the caption available\nmax_length = max(len(ucap.split()) for ucap in data['urdu_caption'])\nmax_length","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:34.875980Z","iopub.execute_input":"2024-07-22T00:42:34.876355Z","iopub.status.idle":"2024-07-22T00:42:35.481757Z","shell.execute_reply.started":"2024-07-22T00:42:34.876319Z","shell.execute_reply":"2024-07-22T00:42:35.480805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the Data","metadata":{}},{"cell_type":"markdown","source":"5 Captions per image\n* Train = 70% (562089)\n* Validation = 20% (140523)\n* Test = 10% (78069)\n\n1 caption per image:\n* Train = 72% (112370)\n* Validation = 18% (28093)\n* Test = 10% (15607)","metadata":{}},{"cell_type":"code","source":"X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(data['image'], data['urdu_caption'], \n                                                                test_size = 0.10, random_state = 30, shuffle = True)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_train_valid, Y_train_valid, \n                                                      test_size = 0.2, random_state = 30, shuffle = True)\n\nprint('Train set size:', X_train.shape)\nprint('Validation set size:', X_valid.shape)\nprint('Test set size:', X_test.shape)\nprint('Total data size', X_train.shape[0] + X_valid.shape[0] + X_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:35.482876Z","iopub.execute_input":"2024-07-22T00:42:35.483162Z","iopub.status.idle":"2024-07-22T00:42:35.667133Z","shell.execute_reply.started":"2024-07-22T00:42:35.483139Z","shell.execute_reply":"2024-07-22T00:42:35.666217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"markdown","source":"* A model architecture that will combine visual information from images with textual information from partial captions to generate captions for images.\n* The model will learn to generate captions based on both the visual content and the context provided by the captions.","metadata":{}},{"cell_type":"code","source":"strategy = tf.distribute.MultiWorkerMirroredStrategy()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:35.668354Z","iopub.execute_input":"2024-07-22T00:42:35.668646Z","iopub.status.idle":"2024-07-22T00:42:35.953728Z","shell.execute_reply.started":"2024-07-22T00:42:35.668622Z","shell.execute_reply":"2024-07-22T00:42:35.952977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n# image feature extractor model\n    inputs1 = Input(shape = (1000,))             # defines an input layer for the image features.\n    fe1 = BatchNormalization()(inputs1)          # applies batch normalization to the input image features.\n    fe2 = Dense(512, activation = 'relu')(fe1)   # applies a dense layer with 512 units and ReLU activation to the batch-normalized image features.\n\n    # partial caption sequence model\n    inputs2 = Input(shape = (max_length,))       # defines an input layer for the partial caption sequences.\n    se1 = Embedding(vocab_size, 512)(inputs2)    # embeds the input sequences into dense vectors of size 512. \n                                                 # this layer uses an embedding matrix with a vocabulary size of vocab_size.\n    se2 = BatchNormalization()(se1)              # applies batch normalization to the embedded sequences.\n    se3 = GRU(256)(se2)          # applies a GRU layer with 256 units to the batch-normalized embedded sequences.\n\n    # decoder model\n    decoder = Concatenate()([fe2, se3])                  # concatenates output features from the image extractor and partial caption sequence models.\n    decoder2 = Dense(256, activation = 'relu')(decoder)  # applies a dense layer with 512 units and ReLU activation to the concatenated features.\n    outputs = Dense(vocab_size,                          # applies a dense layer with vocab_size units and softmax activation to produce the output\n                    activation = 'softmax')(decoder2)    # probability distribution over the vocabulary.\n\n    # merge 2 networks\n    model = Model(inputs = [inputs1, inputs2], outputs = outputs)\n\n    optimizer = Adam(learning_rate = 0.005,\n                     clipvalue = 5.0)          # technique used to limit the magnitude of gradients during training.\n                                               # helps stabilize the training process, especially with exploding gradients.\n\n    model.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-07-14T09:20:09.755900Z","iopub.execute_input":"2024-07-14T09:20:09.756193Z","iopub.status.idle":"2024-07-14T09:20:10.535566Z","shell.execute_reply.started":"2024-07-14T09:20:09.756167Z","shell.execute_reply":"2024-07-14T09:20:10.534631Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The merged model combines the image feature extractor model and the partial caption sequence model into a single model.<br>\nIt takes both the image features (inputs1) and the partial caption sequences (inputs2) as inputs and produces the output probabilities over the vocabulary (outputs).","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T09:20:10.536802Z","iopub.execute_input":"2024-07-14T09:20:10.537118Z","iopub.status.idle":"2024-07-14T09:20:10.566322Z","shell.execute_reply.started":"2024-07-14T09:20:10.537094Z","shell.execute_reply":"2024-07-14T09:20:10.565490Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T09:20:10.567454Z","iopub.execute_input":"2024-07-14T09:20:10.567811Z","iopub.status.idle":"2024-07-14T09:20:11.288362Z","shell.execute_reply.started":"2024-07-14T09:20:10.567780Z","shell.execute_reply":"2024-07-14T09:20:11.287525Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# create data generator to get data in batch (avoids session crash)\ndef data_generator(data, features, tokenizer, max_length, vocab_size, batch_size):\n    # loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data.head()['image'].tolist():\n            n += 1\n            captions = data[data['image'] == key]['urdu_caption']   # ek img k sary captions, 'captions' is a list\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pairs\n                    in_seq, out_seq = seq[:i], seq[i]    # for predicting next word for a given word\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]   # to make length of all captions same by appending zeros\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]   # to tell the index where the word is stored in tokenizer\n                    \n                    # store the sequences (all these are 2D lists now)\n                    X1.append(features[key.split('.')[0]][0].cpu().numpy())\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield (X1, X2), y\n                X1, X2, y = list(), list(), list()\n                n = 0\n    return np.array(X1), np.array(X2), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:35.956319Z","iopub.execute_input":"2024-07-22T00:42:35.956656Z","iopub.status.idle":"2024-07-22T00:42:35.967119Z","shell.execute_reply.started":"2024-07-22T00:42:35.956630Z","shell.execute_reply":"2024-07-22T00:42:35.966201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = {'image': X_train.tolist(), 'urdu_caption': Y_train.tolist()}\ntrain_set = pd.DataFrame(train_set)\ntrain_set","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:35.968422Z","iopub.execute_input":"2024-07-22T00:42:35.968791Z","iopub.status.idle":"2024-07-22T00:42:36.137625Z","shell.execute_reply.started":"2024-07-22T00:42:35.968761Z","shell.execute_reply":"2024-07-22T00:42:36.136623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 60\nsteps = len(X_train) // batch_size\n\n# Define a ModelCheckpoint callback\n# Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = 'model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n    \n    # fit for one epoch\n    model.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T12:08:55.466914Z","iopub.execute_input":"2024-07-20T12:08:55.467749Z","iopub.status.idle":"2024-07-20T12:08:55.719539Z","shell.execute_reply.started":"2024-07-20T12:08:55.467718Z","shell.execute_reply":"2024-07-20T12:08:55.718333Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('Trained_YOLO_GRU_3c1e.h5')         # write no. of captions used and epochs done","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:27:43.468067Z","iopub.execute_input":"2024-07-14T16:27:43.468712Z","iopub.status.idle":"2024-07-14T16:27:43.835391Z","shell.execute_reply.started":"2024-07-14T16:27:43.468661Z","shell.execute_reply":"2024-07-14T16:27:43.834612Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate = 0.005,\n                     clipvalue = 5.0)          # technique used to limit the magnitude of gradients during training.\n                                               # helps stabilize the training process, especially with exploding gradients.","metadata":{"execution":{"iopub.status.busy":"2024-07-22T00:42:36.138799Z","iopub.execute_input":"2024-07-22T00:42:36.139053Z","iopub.status.idle":"2024-07-22T00:42:36.146211Z","shell.execute_reply.started":"2024-07-22T00:42:36.139032Z","shell.execute_reply":"2024-07-22T00:42:36.145260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1 = load_model('/kaggle/input/yolo-gru-m1/Trained_YOLO_GRU_3c1e.h5')\nm1.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-07-17T07:37:02.657757Z","iopub.execute_input":"2024-07-17T07:37:02.658179Z","iopub.status.idle":"2024-07-17T07:37:03.981868Z","shell.execute_reply.started":"2024-07-17T07:37:02.658146Z","shell.execute_reply":"2024-07-17T07:37:03.980945Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 60\nsteps = len(X_train) // batch_size\n\n# Define a ModelCheckpoint callback\n# Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = 'model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n    \n    # fit for one epoch\n    m1.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T07:37:03.983130Z","iopub.execute_input":"2024-07-17T07:37:03.983461Z","iopub.status.idle":"2024-07-17T14:23:43.166374Z","shell.execute_reply.started":"2024-07-17T07:37:03.983430Z","shell.execute_reply":"2024-07-17T14:23:43.165304Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1.save('Trained_YOLO_GRU_3c2e.h5')         # write no. of captions used and epochs done","metadata":{"execution":{"iopub.status.busy":"2024-07-17T14:23:43.168297Z","iopub.execute_input":"2024-07-17T14:23:43.168624Z","iopub.status.idle":"2024-07-17T14:23:43.543242Z","shell.execute_reply.started":"2024-07-17T14:23:43.168590Z","shell.execute_reply":"2024-07-17T14:23:43.542434Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m2 = load_model('/kaggle/input/yolo-gru-m2/Trained_YOLO_GRU_3c2e.h5')\nm2.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:10:09.860351Z","iopub.execute_input":"2024-07-21T16:10:09.860724Z","iopub.status.idle":"2024-07-21T16:10:11.246449Z","shell.execute_reply.started":"2024-07-21T16:10:09.860697Z","shell.execute_reply":"2024-07-21T16:10:11.245364Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 60\nsteps = len(X_train) // batch_size\n\n# Define a ModelCheckpoint callback\n# Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = 'model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n\n    # fit for one epoch\n    m2.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T08:54:26.510039Z","iopub.execute_input":"2024-07-21T08:54:26.510294Z","iopub.status.idle":"2024-07-21T15:28:55.709273Z","shell.execute_reply.started":"2024-07-21T08:54:26.510272Z","shell.execute_reply":"2024-07-21T15:28:55.708236Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m2.save('Trained_YOLO_GRU_3c3e.h5')         # write no. of captions used and epochs done","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:06:21.731309Z","iopub.execute_input":"2024-07-21T16:06:21.731685Z","iopub.status.idle":"2024-07-21T16:06:22.096677Z","shell.execute_reply.started":"2024-07-21T16:06:21.731660Z","shell.execute_reply":"2024-07-21T16:06:22.095750Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m3 = load_model('Trained_YOLO_GRU_3c3e.h5')\nm3.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:14:19.618198Z","iopub.execute_input":"2024-07-21T16:14:19.618545Z","iopub.status.idle":"2024-07-21T16:14:19.867397Z","shell.execute_reply.started":"2024-07-21T16:14:19.618515Z","shell.execute_reply":"2024-07-21T16:14:19.866627Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 60\nsteps = len(X_train) // batch_size\n\n# Define a ModelCheckpoint callback\n# Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = 'model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n\n    # fit for one epoch\n    m3.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:14:19.868491Z","iopub.execute_input":"2024-07-21T16:14:19.868769Z","iopub.status.idle":"2024-07-21T22:45:44.274001Z","shell.execute_reply.started":"2024-07-21T16:14:19.868745Z","shell.execute_reply":"2024-07-21T22:45:44.273165Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m3.save('Trained_YOLO_GRU_3c4e.h5')         # write no. of captions used and epochs done","metadata":{"execution":{"iopub.status.busy":"2024-07-21T22:45:44.276682Z","iopub.execute_input":"2024-07-21T22:45:44.277355Z","iopub.status.idle":"2024-07-21T22:45:44.644370Z","shell.execute_reply.started":"2024-07-21T22:45:44.277319Z","shell.execute_reply":"2024-07-21T22:45:44.643318Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m4 = load_model('Trained_YOLO_GRU_3c4e.h5')\nm4.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:17:29.285799Z","iopub.execute_input":"2024-07-22T01:17:29.286179Z","iopub.status.idle":"2024-07-22T01:17:29.569688Z","shell.execute_reply.started":"2024-07-22T01:17:29.286152Z","shell.execute_reply":"2024-07-22T01:17:29.568900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 60\nsteps = len(X_train) // batch_size\n\n# Define a ModelCheckpoint callback\n# Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = 'model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n\n    # fit for one epoch\n    m4.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:17:33.911280Z","iopub.execute_input":"2024-07-22T01:17:33.912056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m4.save('Trained_YOLO_GRU_3c5e.h5')         # write no. of captions used and epochs done","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate a Caption using the Model","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word,index, in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:28:17.665773Z","iopub.execute_input":"2024-07-14T16:28:17.666140Z","iopub.status.idle":"2024-07-14T16:28:17.671149Z","shell.execute_reply.started":"2024-07-14T16:28:17.666114Z","shell.execute_reply":"2024-07-14T16:28:17.670259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate caption for an image\ndef predict_caption(model, image, tokenizer, max_length):\n    # add start tag for generation process\n    in_text = 'endseq'\n    # iterate over the max length of sequence\n    for i in range(max_length):\n        # encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        \n        # pad the sequence\n        sequence = pad_sequences([sequence], max_length)\n        \n        # predict next word\n        yhat = model.predict([image, sequence], verbose=0)\n        \n        # get index with high probability\n        yhat = np.argmax(yhat)\n        \n        # convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        \n        # stop if word not found\n        if word is None:\n            break\n        # append word as input for generating next word\n        in_text += \" \" + word\n        \n        # stop if we reach end tag\n        if word == 'startseq':\n            break\n    return in_text","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:28:17.981498Z","iopub.execute_input":"2024-07-14T16:28:17.981867Z","iopub.status.idle":"2024-07-14T16:28:17.989020Z","shell.execute_reply.started":"2024-07-14T16:28:17.981839Z","shell.execute_reply":"2024-07-14T16:28:17.988153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validate with test data\nactual, predicted = list(), list()\n\nfor key in tqdm(X_test.iloc[:100]):\n    # get actual caption\n    captions = data[data['image'] == key]['urdu_caption']\n    \n    # predict the caption for image\n    y_pred = predict_caption(model, features[key.split('.')[0]], tokenizer, max_length) \n    \n    # split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    \n    # append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n    \n# calcuate BLEU score\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(0.75, 0.25)))\n# print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:29:49.097233Z","iopub.execute_input":"2024-07-14T16:29:49.097600Z","iopub.status.idle":"2024-07-14T16:29:49.781984Z","shell.execute_reply.started":"2024-07-14T16:29:49.097571Z","shell.execute_reply":"2024-07-14T16:29:49.780738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_caption(image_name):\n    # load the image\n    image_id = image_name.split('.')[0]\n    img_path = \"/kaggle/input/test-images/\" + image_name\n    image = Image.open(img_path)\n\n    # Predict caption for image\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)[8:][:-6]\n    print('--------------------Predicted--------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:32:36.570575Z","iopub.execute_input":"2024-07-14T16:32:36.571241Z","iopub.status.idle":"2024-07-14T16:32:36.576824Z","shell.execute_reply.started":"2024-07-14T16:32:36.571208Z","shell.execute_reply":"2024-07-14T16:32:36.575952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"img000000.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:32:39.313356Z","iopub.execute_input":"2024-07-14T16:32:39.313963Z","iopub.status.idle":"2024-07-14T16:32:39.510646Z","shell.execute_reply.started":"2024-07-14T16:32:39.313932Z","shell.execute_reply":"2024-07-14T16:32:39.509358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}