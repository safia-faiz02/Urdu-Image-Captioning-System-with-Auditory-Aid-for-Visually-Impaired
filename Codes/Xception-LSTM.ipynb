{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7888918,"sourceType":"datasetVersion","datasetId":4631396},{"sourceId":7889090,"sourceType":"datasetVersion","datasetId":4631525},{"sourceId":7980273,"sourceType":"datasetVersion","datasetId":4696823},{"sourceId":8932717,"sourceType":"datasetVersion","datasetId":5373896}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom textwrap import wrap\n\nimport os\nimport gc       # garbage collection: for memory allocation and deallocation\nimport pickle\nimport keras\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model, Sequence\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, BatchNormalization, Concatenate\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# from nltk.translate.bleu_score import corpus_bleu\n\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:20.534904Z","iopub.execute_input":"2024-07-29T16:28:20.535558Z","iopub.status.idle":"2024-07-29T16:28:33.410462Z","shell.execute_reply.started":"2024-07-29T16:28:20.535523Z","shell.execute_reply":"2024-07-29T16:28:33.409648Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-29 16:28:22.957509: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-29 16:28:22.957639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-29 16:28:23.066207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"BLEU score implementation.\"\"\"\nimport math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\n\nfrom nltk.util import ngrams\n\n\nclass Fraction(_Fraction):\n    \"\"\"Fraction with _normalize=False support for 3.12\"\"\"\n\n    def __new__(cls, numerator=0, denominator=None, _normalize=False):\n        if sys.version_info >= (3, 12):\n            self = super().__new__(cls, numerator, denominator)\n        else:\n            self = super().__new__(cls, numerator, denominator, _normalize=_normalize)\n        self._normalize = _normalize\n        self._original_numerator = numerator\n        self._original_denominator = denominator\n        return self\n\n    @property\n    def numerator(self):\n        if not self._normalize:\n            return self._original_numerator\n        return super().numerator\n\n    @property\n    def denominator(self):\n        if not self._normalize:\n            return self._original_denominator\n        return super().denominator\n\ndef sentence_bleu(\n    references,\n    hypothesis,\n    weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None,\n    auto_reweigh=False,):\n    return corpus_bleu([references], [hypothesis], weights, smoothing_function, auto_reweigh)\n\ndef corpus_bleu(\n    list_of_references,\n    hypotheses,\n    weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None,\n    auto_reweigh=False,):\n    # Before proceeding to compute BLEU, perform sanity checks.\n\n    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n    hyp_lengths, ref_lengths = 0, 0\n\n    assert len(list_of_references) == len(hypotheses), ()\n\n    try:\n        weights[0][0]\n    except:\n        weights = [weights]\n    max_weight_length = max(len(weight) for weight in weights)\n\n    # Iterate through each hypothesis and their corresponding references.\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        # For each order of ngram, calculate the numerator and\n        # denominator for the corpus-level modified precision.\n        for i in range(1, max_weight_length + 1):\n            p_i = modified_precision(references, hypothesis, i)\n            p_numerators[i] += p_i.numerator\n            p_denominators[i] += p_i.denominator\n\n        # Calculate the hypothesis length and the closest reference length.\n        # Adds them to the corpus-level hypothesis and reference counts.\n        hyp_len = len(hypothesis)\n        hyp_lengths += hyp_len\n        ref_lengths += closest_ref_length(references, hyp_len)\n\n    # Calculate corpus-level brevity penalty.\n    bp = brevity_penalty(ref_lengths, hyp_lengths)\n\n    # Collects the various precision values for the different ngram orders.\n    p_n = [Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n        for i in range(1, max_weight_length + 1)]\n\n    # Returns 0 if there's no matching n-grams\n    # We only need to check for p_numerators[1] == 0, since if there's\n    # no unigrams, there won't be any higher order ngrams.\n    if p_numerators[1] == 0:\n        return 0 if len(weights) == 1 else [0] * len(weights)\n\n    # If there's no smoothing, set use method0 from SmoothinFunction class.\n    if not smoothing_function:\n        smoothing_function = SmoothingFunction().method0\n    # Smoothen the modified precision.\n    # Note: smoothing_function() may convert values into floats;\n    #       it tries to retain the Fraction object as much as the\n    #       smoothing method allows.\n    p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths)\n\n    bleu_scores = []\n    for weight in weights:\n        # Uniformly re-weighting based on maximum hypothesis lengths if largest\n        # order of n-grams < 4 and weights is set at default.\n        if auto_reweigh:\n            if hyp_lengths < 4 and weight == (0.25, 0.25, 0.25, 0.25):\n                weight = (1 / hyp_lengths,) * hyp_lengths\n\n        s = (w_i * math.log(p_i) for w_i, p_i in zip(weight, p_n) if p_i > 0)\n        s = bp * math.exp(math.fsum(s))\n        bleu_scores.append(s)\n    return bleu_scores[0] if len(weights) == 1 else bleu_scores\n\n\ndef modified_precision(references, hypothesis, n):\n    # Extracts all ngrams in hypothesis\n    # Set an empty Counter if hypothesis is empty.\n    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n    # Extract a union of references' counts.\n    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n    max_counts = {}\n    for reference in references:\n        reference_counts = (Counter(ngrams(reference, n)) if len(reference) >= n else Counter())\n        for ngram in counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    # Assigns the intersection between hypothesis and references' counts.\n    clipped_counts = {ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()}\n\n    numerator = sum(clipped_counts.values())\n    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n    # Usually this happens when the ngram order is > len(reference).\n    denominator = max(1, sum(counts.values()))\n\n    return Fraction(numerator, denominator, _normalize=False)\n\n\ndef closest_ref_length(references, hyp_len):\n    ref_lens = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))\n    return closest_ref_len\n\n\ndef brevity_penalty(closest_ref_len, hyp_len):\n    if hyp_len > closest_ref_len:\n        return 1\n    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n    elif hyp_len == 0:\n        return 0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)\n\n\nclass SmoothingFunction:\n    def __init__(self, epsilon=0.1, alpha=5, k=5):\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.k = k\n\n    def method0(self, p_n, *args, **kwargs):\n        \"\"\"\n        No smoothing.\n        \"\"\"\n        p_n_new = []\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator != 0:\n                p_n_new.append(p_i)\n            else:\n                _msg = str().format(i + 1)\n                warnings.warn(_msg)\n                # When numerator==0 where denonminator==0 or !=0, the result\n                # for the precision score should be equal to 0 or undefined.\n                # Due to BLEU geometric mean computation in logarithm space,\n                # we we need to take the return sys.float_info.min such that\n                # math.log(sys.float_info.min) returns a 0 precision score.\n                p_n_new.append(sys.float_info.min)\n        return p_n_new\n\n    def method1(self, p_n, *args, **kwargs):\n        \"\"\"\n        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n        \"\"\"\n        return [(p_i.numerator + self.epsilon) / p_i.denominator\n            if p_i.numerator == 0\n            else p_i\n            for p_i in p_n]\n\n    def method2(self, p_n, *args, **kwargs):\n        return [Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n            if i != 0\n            else p_n[0]\n            for i in range(len(p_n))]\n\n    def method3(self, p_n, *args, **kwargs):\n        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0:\n                p_n[i] = 1 / (2**incvnt * p_i.denominator)\n                incvnt += 1\n        return p_n\n\n    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        incvnt = 1\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        for i, p_i in enumerate(p_n):\n            if p_i.numerator == 0 and hyp_len > 1:\n                # incvnt = i + 1 * self.k / math.log(\n                #     hyp_len\n                # )  # Note that this K is different from the K from NIST.\n                # p_n[i] = incvnt / p_i.denominator\\\n                numerator = 1 / (2**incvnt * self.k / math.log(hyp_len))\n                p_n[i] = numerator / p_i.denominator\n                incvnt += 1\n        return p_n\n\n    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        m = {}\n        # Requires an precision value for an addition ngram order.\n        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n        m[-1] = p_n[0] + 1\n        for i, p_i in enumerate(p_n):\n            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n            m[i] = p_n[i]\n        return p_n\n\n    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        # This smoothing only works when p_1 and p_2 is non-zero.\n        # Raise an error with an appropriate message when the input is too short\n        # to use this smoothing technique.\n        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n        for i, p_i in enumerate(p_n):\n            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n                continue\n            else:\n                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n                # No. of ngrams in translation that matches the reference.\n                m = p_i.numerator\n                # No. of ngrams in translation.\n                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n                # Calculates the interpolated precision.\n                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n        return p_n\n\n    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n        \"\"\"\n        Smoothing method 7:\n        Interpolates methods 4 and 5.\n        \"\"\"\n        hyp_len = hyp_len if hyp_len else len(hypothesis)\n        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n        return p_n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:33.412509Z","iopub.execute_input":"2024-07-29T16:28:33.413162Z","iopub.status.idle":"2024-07-29T16:28:33.943481Z","shell.execute_reply.started":"2024-07-29T16:28:33.413128Z","shell.execute_reply":"2024-07-29T16:28:33.942705Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## For GPU","metadata":{}},{"cell_type":"code","source":"strategy = tf.distribute.MultiWorkerMirroredStrategy()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:33.944575Z","iopub.execute_input":"2024-07-29T16:28:33.944849Z","iopub.status.idle":"2024-07-29T16:28:34.919733Z","shell.execute_reply.started":"2024-07-29T16:28:33.944825Z","shell.execute_reply":"2024-07-29T16:28:34.918911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## For TPU","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.TPUStrategy(tpu)\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Data ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataset3cap/FINAL_COMBINED_DATASET_3CAP.csv')\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:34.921408Z","iopub.execute_input":"2024-07-29T16:28:34.921692Z","iopub.status.idle":"2024-07-29T16:28:37.661837Z","shell.execute_reply.started":"2024-07-29T16:28:34.921667Z","shell.execute_reply":"2024-07-29T16:28:37.660921Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           image                                            caption  \\\n0  img000000.jpg  Closeup of bins of food that include broccoli ...   \n1  img000000.jpg         A bunch of trays that have different food.   \n2  img000000.jpg  Colorful dishes holding meat, vegetables, frui...   \n3  img000001.jpg               A giraffe standing up nearby a tree    \n4  img000001.jpg      A giraffe mother with its baby in the forest.   \n5  img000001.jpg       Two giraffes standing in a tree filled area.   \n6  img000002.jpg  White vase with different colored flowers sitt...   \n7  img000002.jpg         A flower vase is sitting on a porch stand.   \n8  img000002.jpg          a white vase with many flowers on a stage   \n9  img000003.jpg  Zebra reaching its head down to ground where g...   \n\n                                        urdu_caption  \n0  کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی اور رو...  \n1       ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے ہیں۔  \n2  رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھل اور ر...  \n3                ایک زرافہ ایک درخت کے قریب کھڑا ہے۔  \n4        ایک زرافے کی ماں اپنے بچے کے ساتھ جنگل میں۔  \n5        دو زرافے درختوں سے بھرے علاقے میں کھڑے ہیں۔  \n6  سفید گلدان جس کے اندر مختلف رنگوں کے پھول بیٹھ...  \n7           پورچ اسٹینڈ پر پھولوں کا گلدان بیٹھا ہے۔  \n8  ایک اسٹیج پر بہت سے پھولوں کے ساتھ ایک سفید گلدان  \n9  زیبرا اپنا سر نیچے زمین تک پہنچا رہا ہے جہاں گ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>urdu_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img000000.jpg</td>\n      <td>Closeup of bins of food that include broccoli ...</td>\n      <td>کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی اور رو...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img000000.jpg</td>\n      <td>A bunch of trays that have different food.</td>\n      <td>ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے ہیں۔</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img000000.jpg</td>\n      <td>Colorful dishes holding meat, vegetables, frui...</td>\n      <td>رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھل اور ر...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe standing up nearby a tree</td>\n      <td>ایک زرافہ ایک درخت کے قریب کھڑا ہے۔</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe mother with its baby in the forest.</td>\n      <td>ایک زرافے کی ماں اپنے بچے کے ساتھ جنگل میں۔</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>img000001.jpg</td>\n      <td>Two giraffes standing in a tree filled area.</td>\n      <td>دو زرافے درختوں سے بھرے علاقے میں کھڑے ہیں۔</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>img000002.jpg</td>\n      <td>White vase with different colored flowers sitt...</td>\n      <td>سفید گلدان جس کے اندر مختلف رنگوں کے پھول بیٹھ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>img000002.jpg</td>\n      <td>A flower vase is sitting on a porch stand.</td>\n      <td>پورچ اسٹینڈ پر پھولوں کا گلدان بیٹھا ہے۔</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>img000002.jpg</td>\n      <td>a white vase with many flowers on a stage</td>\n      <td>ایک اسٹیج پر بہت سے پھولوں کے ساتھ ایک سفید گلدان</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>img000003.jpg</td>\n      <td>Zebra reaching its head down to ground where g...</td>\n      <td>زیبرا اپنا سر نیچے زمین تک پہنچا رہا ہے جہاں گ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Display Images with Captions","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# def readImage(path, img_size = 224):\n#     img = load_img(path, color_mode = 'rgb', target_size = (img_size, img_size))\n#     img = img_to_array(img)\n#     img = img/255.\n#     return img\n\n# # THIS IS NOT NECESSARY, URDU CAPTIONS THEK DISPLAY NH HO RHE\n# def display_images(temp_df):   \n#     temp_df = temp_df.reset_index(drop = True)\n#     plt.figure(figsize = (20, 20))\n#     n = 0\n#     for i in range(156070 * 5):  \n#         n += 1\n#         plt.subplot(156070, 5, n)  \n#         plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n\n#         image = readImage(f\"/kaggle/input/images/Images/{temp_df.image[i]}\")\n#         plt.imshow(image)\n#         plt.title(\"\\n\".join(wrap(temp_df.urdu_caption[i][-1::-1], 20)))\n#         plt.axis(\"off\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display_images(data)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model to Extract Features","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# # Load the Model\n# cnn = Xception()\n\n# # Restructure model \n# cnn = Model(inputs = cnn.inputs,               # specify inputs to be the same as Xception      \n#             outputs = cnn.layers[-2].output    # specify 2nd last layer as output which extracts the features (last layer is final classification layer)\n#            )\n\n# # Summerize\n# display(cnn.summary())","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Image Features","metadata":{}},{"cell_type":"code","source":"# features = {}\n# directory = '/kaggle/input/images/Images'\n\n# for img_name in tqdm(os.listdir(directory)): \n    \n#     # load the image from file\n#     img_path = directory + '/' + img_name\n#     image = load_img(img_path, target_size = (299, 299))\n#     image = img_to_array(image)                # convert image pixels to numpy array\n    \n#     # reshape data for model\n#     image = image.reshape((1, image.shape[0],  # width \n#                            image.shape[1],     # height\n#                            image.shape[2]      # channels (3 because of rgb)\n#                           ))\n\n#     image = preprocess_input(image)            # preprocess image for Xception\n#     feature = cnn.predict(image, verbose=0)    # extract features\n#     image_id = img_name.split('.')[0]          # get image ID\n#     features[image_id] = feature               # store feature  (size = 2048)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # store features in pickle\n# pickle.dump(features, open('/kaggle/input/pklfile/ImgFeatures.pkl', 'wb'))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load features from pickle\nwith open('/kaggle/input/pklfile/ImgFeatures.pkl', 'rb') as f:    \n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:32:55.568357Z","iopub.execute_input":"2024-07-11T17:32:55.569264Z","iopub.status.idle":"2024-07-11T17:33:05.083175Z","shell.execute_reply.started":"2024-07-11T17:32:55.569233Z","shell.execute_reply":"2024-07-11T17:33:05.081843Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-02T09:01:16.426810Z","iopub.execute_input":"2024-06-02T09:01:16.427261Z","iopub.status.idle":"2024-06-02T09:01:16.431522Z","shell.execute_reply.started":"2024-06-02T09:01:16.427228Z","shell.execute_reply":"2024-06-02T09:01:16.430660Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess captions","metadata":{}},{"cell_type":"code","source":"def preprocessCaption(df):\n    df['urdu_caption'] = 'endseq ' + df['urdu_caption'] + ' startseq'\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:48.561558Z","iopub.execute_input":"2024-07-29T16:28:48.561912Z","iopub.status.idle":"2024-07-29T16:28:48.566510Z","shell.execute_reply.started":"2024-07-29T16:28:48.561884Z","shell.execute_reply":"2024-07-29T16:28:48.565572Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data = data.apply(preprocessCaption, axis = 1)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:49.160503Z","iopub.execute_input":"2024-07-29T16:28:49.161339Z","iopub.status.idle":"2024-07-29T16:29:23.199022Z","shell.execute_reply.started":"2024-07-29T16:28:49.161298Z","shell.execute_reply":"2024-07-29T16:29:23.198112Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                image                                            caption  \\\n0       img000000.jpg  Closeup of bins of food that include broccoli ...   \n1       img000000.jpg         A bunch of trays that have different food.   \n2       img000000.jpg  Colorful dishes holding meat, vegetables, frui...   \n3       img000001.jpg               A giraffe standing up nearby a tree    \n4       img000001.jpg      A giraffe mother with its baby in the forest.   \n...               ...                                                ...   \n468205  img155817.jpg    a small sheep standing on top of a grass field    \n468206  img155818.jpg      a small bird sitting on top of a tree branch    \n468207  img155818.jpg              an owl perched on a branch in a tree    \n468208  img155819.jpg  a small brown elephant with brown fur eating g...   \n468209  img155819.jpg  a brown horse eating grass with a green backgr...   \n\n                                             urdu_caption  \n0       endseq کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی...  \n1       endseq ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے...  \n2       endseq رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھ...  \n3       endseq ایک زرافہ ایک درخت کے قریب کھڑا ہے۔ sta...  \n4       endseq ایک زرافے کی ماں اپنے بچے کے ساتھ جنگل ...  \n...                                                   ...  \n468205  endseq ایک چھوٹی بھیڑ گھاس کے میدان کے اوپر کھ...  \n468206  endseq درخت کی شاخ کے اوپر بیٹھا ہوا ایک چھوٹا...  \n468207  endseq ایک اُلّو درخت کی شاخ پر بیٹھا ہے۔ star...  \n468208  endseq بھوری کھال کے ساتھ ایک چھوٹا بھورا ہاتھ...  \n468209  endseq ایک بھورا گھوڑا سبز پس منظر کے ساتھ گھا...  \n\n[468210 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>urdu_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img000000.jpg</td>\n      <td>Closeup of bins of food that include broccoli ...</td>\n      <td>endseq کھانے کے ڈبوں کا کلوز اپ جس میں بروکولی...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img000000.jpg</td>\n      <td>A bunch of trays that have different food.</td>\n      <td>endseq ٹرے کا ایک گروپ جس میں مختلف کھانے ہوتے...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img000000.jpg</td>\n      <td>Colorful dishes holding meat, vegetables, frui...</td>\n      <td>endseq رنگ برنگے پکوان جن میں گوشت، سبزیاں، پھ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe standing up nearby a tree</td>\n      <td>endseq ایک زرافہ ایک درخت کے قریب کھڑا ہے۔ sta...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img000001.jpg</td>\n      <td>A giraffe mother with its baby in the forest.</td>\n      <td>endseq ایک زرافے کی ماں اپنے بچے کے ساتھ جنگل ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>468205</th>\n      <td>img155817.jpg</td>\n      <td>a small sheep standing on top of a grass field</td>\n      <td>endseq ایک چھوٹی بھیڑ گھاس کے میدان کے اوپر کھ...</td>\n    </tr>\n    <tr>\n      <th>468206</th>\n      <td>img155818.jpg</td>\n      <td>a small bird sitting on top of a tree branch</td>\n      <td>endseq درخت کی شاخ کے اوپر بیٹھا ہوا ایک چھوٹا...</td>\n    </tr>\n    <tr>\n      <th>468207</th>\n      <td>img155818.jpg</td>\n      <td>an owl perched on a branch in a tree</td>\n      <td>endseq ایک اُلّو درخت کی شاخ پر بیٹھا ہے۔ star...</td>\n    </tr>\n    <tr>\n      <th>468208</th>\n      <td>img155819.jpg</td>\n      <td>a small brown elephant with brown fur eating g...</td>\n      <td>endseq بھوری کھال کے ساتھ ایک چھوٹا بھورا ہاتھ...</td>\n    </tr>\n    <tr>\n      <th>468209</th>\n      <td>img155819.jpg</td>\n      <td>a brown horse eating grass with a green backgr...</td>\n      <td>endseq ایک بھورا گھوڑا سبز پس منظر کے ساتھ گھا...</td>\n    </tr>\n  </tbody>\n</table>\n<p>468210 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.iloc[87014, 2]","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:23.200831Z","iopub.execute_input":"2024-07-29T16:29:23.201135Z","iopub.status.idle":"2024-07-29T16:29:23.207173Z","shell.execute_reply.started":"2024-07-29T16:29:23.201088Z","shell.execute_reply":"2024-07-29T16:29:23.206140Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'endseq شہر میں گاڑیوں کو سرخ بتی پر روکا جاتا ہے۔ startseq'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing the Text","metadata":{}},{"cell_type":"markdown","source":"In Python, indices typically start from 0. However, when working with tokenization in NLP, we often reserve index 0 for special tokens, such as padding tokens or unknown tokens.\n* **Padding Token:** In many NLP tasks, sequences of words or tokens are padded to ensure uniform length. Padding tokens are used to fill in the extra spaces in sequences to make them uniform. Index 0 is usually reserved for the padding token.\n<br><br>\n* **Unknown Token:** This token is used to represent words that are not present in the vocabulary. When a word that is not in the vocabulary is encountered during tokenization, it is replaced by the unknown token. Again, index 0 is often reserved for this purpose.","metadata":{}},{"cell_type":"code","source":"# tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data[\"urdu_caption\"])\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:23.208319Z","iopub.execute_input":"2024-07-29T16:29:23.208635Z","iopub.status.idle":"2024-07-29T16:29:35.129028Z","shell.execute_reply.started":"2024-07-29T16:29:23.208603Z","shell.execute_reply":"2024-07-29T16:29:35.128139Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"27180"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.word_index","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-29T16:29:35.131347Z","iopub.execute_input":"2024-07-29T16:29:35.131620Z","iopub.status.idle":"2024-07-29T16:29:35.168682Z","shell.execute_reply.started":"2024-07-29T16:29:35.131596Z","shell.execute_reply":"2024-07-29T16:29:35.167767Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'endseq': 1,\n 'startseq': 2,\n 'ایک': 3,\n 'کے': 4,\n 'ہے۔': 5,\n 'میں': 6,\n 'پر': 7,\n 'اور': 8,\n 'رہا': 9,\n 'کی': 10,\n 'ساتھ': 11,\n 'ہیں۔': 12,\n 'کا': 13,\n 'سے': 14,\n 'آدمی': 15,\n 'کر': 16,\n 'رہی': 17,\n 'دو': 18,\n 'ہے': 19,\n 'رہے': 20,\n 'کو': 21,\n 'ہوئے': 22,\n 'جس': 23,\n 'عورت': 24,\n 'سفید': 25,\n 'ہوا': 26,\n 'بیٹھا': 27,\n 'کھڑا': 28,\n 'سڑک': 29,\n 'جو': 30,\n 'میز': 31,\n 'پہنے': 32,\n 'بورڈ': 33,\n 'اپنے': 34,\n 'کچھ': 35,\n 'شخص': 36,\n 'اوپر': 37,\n 'رنگ': 38,\n 'لوگ': 39,\n 'بال': 40,\n 'سامنے': 41,\n 'دیکھ': 42,\n 'سرخ': 43,\n 'کھیل': 44,\n 'نوجوان': 45,\n 'لیے': 46,\n 'والی': 47,\n 'گروپ': 48,\n 'سوار': 49,\n 'پکڑے': 50,\n 'پاس': 51,\n 'قریب': 52,\n 'والے': 53,\n 'نیچے': 54,\n 'والا': 55,\n 'پانی': 56,\n 'لوگوں': 57,\n 'کھڑی': 58,\n 'چل': 59,\n 'گھاس': 60,\n 'بیٹھی': 61,\n 'سیاہ': 62,\n 'اس': 63,\n 'ہوئی': 64,\n 'ٹینس': 65,\n 'کھڑے': 66,\n 'بہت': 67,\n 'لڑکا': 68,\n 'ہیں': 69,\n 'قمیض': 70,\n 'باہر': 71,\n 'چھوٹا': 72,\n 'تصویر': 73,\n 'بیس': 74,\n 'بیٹھے': 75,\n 'پلیٹ': 76,\n 'کتا': 77,\n 'لڑکی': 78,\n 'بلی': 79,\n 'میدان': 80,\n 'ٹرین': 81,\n 'دوسرے': 82,\n 'تین': 83,\n 'طرف': 84,\n 'سبز': 85,\n 'سمندر': 86,\n 'عمارت': 87,\n 'سائیکل': 88,\n 'مرد': 89,\n 'بڑا': 90,\n 'ساحل': 91,\n 'فٹ': 92,\n 'نیلے': 93,\n 'بچہ': 94,\n 'روم': 95,\n 'کھانے': 96,\n 'باتھ': 97,\n 'پیچھے': 98,\n 'منظر': 99,\n 'برف': 100,\n 'کھلاڑی': 101,\n 'ٹاپ': 102,\n 'لکڑی': 103,\n 'موٹر': 104,\n 'کئی': 105,\n 'چھوٹی': 106,\n 'اپنی': 107,\n 'فون': 108,\n 'بڑی': 109,\n 'لباس': 110,\n 'شہر': 111,\n 'نے': 112,\n 'بس': 113,\n 'بچے': 114,\n 'کمرے': 115,\n 'کنارے': 116,\n 'پیزا': 117,\n 'نیلی': 118,\n 'بڑے': 119,\n 'ہوائی': 120,\n 'گیند': 121,\n 'اسکیٹ': 122,\n 'چھوٹے': 123,\n 'باورچی': 124,\n 'نشان': 125,\n 'دیوار': 126,\n 'لگا': 127,\n 'گلی': 128,\n 'لیپ': 129,\n 'سرف': 130,\n 'جبکہ': 131,\n 'کرنے': 132,\n 'ملبوس': 133,\n 'ہے،': 134,\n 'بستر': 135,\n 'جب': 136,\n 'سوٹ': 137,\n 'ہاتھ': 138,\n 'مختلف': 139,\n 'پس': 140,\n 'پیلے': 141,\n 'جا': 142,\n 'لے': 143,\n 'کتے': 144,\n 'کھا': 145,\n 'ٹوپی': 146,\n 'ہو': 147,\n 'درخت': 148,\n 'پارک': 149,\n 'کھانا': 150,\n 'کیک': 151,\n 'کمپیوٹر': 152,\n 'کالی': 153,\n 'زیبرا': 154,\n 'ہاتھی': 155,\n 'ٹرک': 156,\n 'چھتری': 157,\n 'خاتون': 158,\n 'گھوڑے': 159,\n 'سکیٹ': 160,\n 'بالوں': 161,\n 'خواتین': 162,\n 'فریسبی': 163,\n 'کھیت': 164,\n 'کھڑکی': 165,\n 'جہاز': 166,\n 'بینچ': 167,\n 'سکی': 168,\n 'چھلانگ': 169,\n 'پاتھ': 170,\n 'گیا': 171,\n 'گزر': 172,\n 'گرد': 173,\n 'گھر': 174,\n 'سیل': 175,\n 'خانے': 176,\n 'درختوں': 177,\n 'اندر': 178,\n 'علاقے': 179,\n 'باڑ': 180,\n 'تیار': 181,\n 'کرتے': 182,\n 'نارنجی': 183,\n 'سنک': 184,\n 'فرش': 185,\n 'وہ': 186,\n 'پہاڑی': 187,\n 'گھڑی': 188,\n 'زمین': 189,\n 'آسمان': 190,\n 'کشتی': 191,\n 'سی': 192,\n 'گلابی': 193,\n 'پتنگ': 194,\n 'کسی': 195,\n 'یہ': 196,\n 'کار': 197,\n 'سامان': 198,\n 'فائر': 199,\n 'بات': 200,\n 'کورٹ': 201,\n 'کرسی': 202,\n 'ٹوائلٹ': 203,\n 'اپنا': 204,\n 'سر': 205,\n 'چار': 206,\n 'بھرے': 207,\n 'بھرا': 208,\n 'کہ': 209,\n 'جیکٹ': 210,\n 'استعمال': 211,\n 'ٹی': 212,\n 'گاڑی': 213,\n 'شیشے': 214,\n 'دن': 215,\n 'ریکیٹ': 216,\n 'ہی': 217,\n 'کام': 218,\n 'لڑکے': 219,\n 'تصویر۔': 220,\n 'کاؤنٹر': 221,\n 'ریچھ': 222,\n 'وے': 223,\n 'پھولوں': 224,\n 'نظر': 225,\n 'ہیں،': 226,\n 'سنو': 227,\n 'ٹریفک': 228,\n 'ہجوم': 229,\n 'جنگل': 230,\n 'صوفے': 231,\n 'شرٹ': 232,\n 'زرافے': 233,\n 'کرتا': 234,\n 'بلے': 235,\n 'وقت': 236,\n 'بھورے': 237,\n 'صاف': 238,\n 'کیلے': 239,\n 'ٹیڈی': 240,\n 'چلا': 241,\n 'کیمرے': 242,\n 'بھری': 243,\n 'پارکنگ': 244,\n 'جوڑا': 245,\n 'دے': 246,\n 'گروپ۔': 247,\n 'بیچ': 248,\n 'کھینچ': 249,\n 'اڑ': 250,\n 'بنا': 251,\n 'اٹھائے': 252,\n 'ٹائی': 253,\n 'سرمئی': 254,\n 'بیئر': 255,\n 'پٹریوں': 256,\n 'دھوپ': 257,\n 'درمیان': 258,\n 'پینٹ': 259,\n 'زرافہ': 260,\n 'بورڈر': 261,\n 'ہوتا': 262,\n 'شراب': 263,\n 'انتظار': 264,\n 'ان': 265,\n 'سینڈوچ': 266,\n 'بھورا': 267,\n 'مسکرا': 268,\n 'گیم': 269,\n 'آدمی۔': 270,\n 'بیگ': 271,\n 'قریبی': 272,\n 'اپ': 273,\n 'پوز': 274,\n 'قسم': 275,\n 'دکھا': 276,\n 'لہر': 277,\n 'بیت': 278,\n 'موٹرسائیکل': 279,\n 'ڈھلوان': 280,\n 'پکڑ': 281,\n 'کوشش': 282,\n 'منہ': 283,\n 'لیٹی': 284,\n 'ہائیڈرنٹ': 285,\n 'دوران': 286,\n 'بجا': 287,\n 'اسٹیشن': 288,\n 'ہوتے': 289,\n 'بچوں': 290,\n 'ٹاور': 291,\n 'دوڑ': 292,\n 'جوڑے': 293,\n 'کیا': 294,\n 'سفر': 295,\n 'جاتا': 296,\n 'ڈھکی': 297,\n 'دوسری': 298,\n 'ارد': 299,\n 'دروازے': 300,\n 'پرندہ': 301,\n 'گائے': 302,\n 'لمبی': 303,\n 'کاٹ': 304,\n 'ٹکڑا': 305,\n 'اینٹوں': 306,\n 'سبزیوں': 307,\n 'رکھا': 308,\n 'کپ': 309,\n 'رات': 310,\n 'اونچی': 311,\n 'خوبصورت': 312,\n 'آئینے': 313,\n 'کیس': 314,\n 'کھمبے': 315,\n 'قطار': 316,\n 'خالی': 317,\n 'سیٹ': 318,\n 'بھوری': 319,\n 'کھلے': 320,\n 'رہنے': 321,\n 'خانہ': 322,\n 'سب': 323,\n 'طرح': 324,\n 'پہاڑ': 325,\n 'سا': 326,\n 'ہاٹ': 327,\n 'دوسرا': 328,\n 'ٹیلی': 329,\n 'کمرہ': 330,\n 'مصروف': 331,\n 'لائٹ': 332,\n 'ٹیک': 333,\n 'رنگین': 334,\n 'پرانی': 335,\n 'اڈے': 336,\n 'شارٹس': 337,\n 'جسم': 338,\n 'مردوں': 339,\n 'دار': 340,\n 'ایشیائی': 341,\n 'چر': 342,\n 'جھول': 343,\n 'چند': 344,\n 'لیٹا': 345,\n 'راستے': 346,\n 'آرام': 347,\n 'کونے': 348,\n 'کوئی': 349,\n 'ہیلمٹ': 350,\n 'جانور': 351,\n 'کافی': 352,\n 'منظر۔': 353,\n 'کچن': 354,\n 'پیالے': 355,\n 'چیز': 356,\n 'سٹاپ': 357,\n 'پار': 358,\n 'ڈبل': 359,\n 'ویڈیو': 360,\n 'بروکولی': 361,\n 'پیش': 362,\n 'بوڑھا': 363,\n 'ریت': 364,\n 'گلدان': 365,\n 'پی': 366,\n 'جامنی': 367,\n 'سو': 368,\n 'اسٹاپ': 369,\n 'گھوڑا': 370,\n 'لڑکیاں': 371,\n 'بازو': 372,\n 'طیارہ': 373,\n 'وی': 374,\n 'ڈاگ': 375,\n 'مسافر': 376,\n 'پیلی': 377,\n 'بالغ': 378,\n 'کرتی': 379,\n 'گھوڑوں': 380,\n 'ریل': 381,\n 'روشنی': 382,\n 'جگہ': 383,\n 'طور': 384,\n 'ڈھیر': 385,\n 'پتھر': 386,\n 'کالا': 387,\n 'جن': 388,\n 'سکینگ': 389,\n 'سنہرے': 390,\n 'عمارتوں': 391,\n 'لمبے': 392,\n 'گندگی': 393,\n 'اڑا': 394,\n 'مارنے': 395,\n 'روشن': 396,\n 'ٹکڑے': 397,\n 'ذریعے': 398,\n 'گلاس': 399,\n 'چہرے': 400,\n 'آنے': 401,\n 'اسے': 402,\n 'دکھایا': 403,\n 'ڈھکے': 404,\n 'الخلا': 405,\n 'دیگر': 406,\n 'رن': 407,\n 'ڈیسک': 408,\n 'بھیڑ': 409,\n 'ریموٹ': 410,\n 'ویژن': 411,\n 'گئی': 412,\n 'شخص۔': 413,\n 'کلاک': 414,\n 'ڈونٹس': 415,\n 'ریمپ': 416,\n 'جینز': 417,\n 'دھاری': 418,\n 'پکڑنے': 419,\n 'کالے': 420,\n 'پرندے': 421,\n 'جمع': 422,\n 'کھلا': 423,\n 'سرفنگ': 424,\n 'بھاگ': 425,\n 'کرتب': 426,\n 'دیکھتے': 427,\n 'رکھے': 428,\n 'یا': 429,\n 'نشان۔': 430,\n 'پھلوں': 431,\n 'گٹار': 432,\n 'پل': 433,\n 'چلنے': 434,\n 'پھینک': 435,\n 'جھیل': 436,\n 'برش': 437,\n 'بغیر': 438,\n 'دکان': 439,\n 'ریفریجریٹر': 440,\n 'کوٹ': 441,\n 'رکھی': 442,\n 'بیڈ': 443,\n 'ٹرے': 444,\n 'پھول': 445,\n 'آلود': 446,\n 'wii': 447,\n 'دونوں': 448,\n 'لوگ۔': 449,\n 'مار': 450,\n 'تیاری': 451,\n 'گوشت': 452,\n 'پنیر': 453,\n 'چٹان': 454,\n 'ٹب': 455,\n 'پکڑا': 456,\n 'برتن': 457,\n 'بند': 458,\n 'کاغذ': 459,\n 'دکھائی': 460,\n 'دریا': 461,\n 'یونیفارم': 462,\n 'ڈسپلے': 463,\n 'اشیاء': 464,\n 'سرفر': 465,\n 'ساتھ۔': 466,\n 'ہوتی': 467,\n 'غسل': 468,\n 'حصہ': 469,\n 'بورڈنگ': 470,\n 'بھیڑوں': 471,\n 'کتاب': 472,\n 'ڈیکر': 473,\n 'ہاتھیوں': 474,\n 'نہیں': 475,\n 'کپڑے': 476,\n 'پڑھ': 477,\n 'روم۔': 478,\n 'پرانے': 479,\n 'بنچ': 480,\n 'کاروں': 481,\n 'گھوم': 482,\n 'جیٹ': 483,\n 'جوتے': 484,\n 'بوتل': 485,\n 'عورت۔': 486,\n 'ریستوراں': 487,\n 'سیب': 488,\n 'آگے': 489,\n 'شدہ': 490,\n 'کراس': 491,\n 'دور': 492,\n 'تندور': 493,\n 'ٹوکری': 494,\n 'نشانات': 495,\n 'بیٹ': 496,\n 'پولیس': 497,\n 'آلات': 498,\n 'افراد': 499,\n 'لہروں': 500,\n 'بارش': 501,\n 'شاور': 502,\n 'ڈونٹ': 503,\n 'جیسے': 504,\n 'تھا۔': 505,\n 'لطف': 506,\n 'ٹریک': 507,\n 'چوٹی': 508,\n 'تک': 509,\n 'لائن': 510,\n 'ٹیبل': 511,\n 'دھاتی': 512,\n 'ہونے': 513,\n 'لونگ': 514,\n 'کاریں': 515,\n 'ریوڑ': 516,\n 'پلیٹ۔': 517,\n 'کھلونا': 518,\n 'واک': 519,\n 'اندوز': 520,\n 'چڑیا': 521,\n 'اسٹینڈ': 522,\n 'ریکٹ': 523,\n 'جانے': 524,\n 'پہن': 525,\n 'سائیکلوں': 526,\n 'کمبل': 527,\n 'چھتریوں': 528,\n 'پکڑی': 529,\n 'جھک': 530,\n 'تالاب': 531,\n 'کھلی': 532,\n 'زیادہ': 533,\n 'پیدل': 534,\n 'ٹرین۔': 535,\n 'چمکدار': 536,\n 'پلاسٹک': 537,\n 'حصے': 538,\n 'اڑتا': 539,\n 'آدمی،': 540,\n 'پھل': 541,\n 'چلتے': 542,\n 'جانوروں': 543,\n 'فارم': 544,\n 'کلوز': 545,\n 'بیرونی': 546,\n 'سواری': 547,\n 'شیلف': 548,\n 'چوراہے': 549,\n 'چھت': 550,\n 'و': 551,\n 'پین': 552,\n 'اسکرین': 553,\n 'پڑا': 554,\n 'چاندی': 555,\n 'گائیں': 556,\n 'ابر': 557,\n 'پانچ': 558,\n 'پاؤں': 559,\n 'باکس': 560,\n 'مانیٹر': 561,\n 'باز': 562,\n 'سیڑھیوں': 563,\n 'کھیلتے': 564,\n 'لٹکا': 565,\n 'چال': 566,\n 'چاقو': 567,\n 'برفیلے': 568,\n 'اشارہ': 569,\n 'برفیلی': 570,\n 'صحن': 571,\n 'ٹیم': 572,\n 'بے': 573,\n 'آ': 574,\n 'تعمیراتی': 575,\n 'شامل': 576,\n 'اسٹریٹ': 577,\n 'خانہ۔': 578,\n 'سبزیاں': 579,\n 'بوڑھی': 580,\n 'کرسیاں': 581,\n 'بیٹھ': 582,\n 'کود': 583,\n 'فرنیچر': 584,\n 'چشمہ': 585,\n 'روٹی': 586,\n 'چولہے': 587,\n 'اسکیئر': 588,\n 'ٹائل': 589,\n 'بورڈز': 590,\n 'لائٹس': 591,\n 'پرانا': 592,\n 'ٹین': 593,\n 'سنتری': 594,\n 'بازار': 595,\n 'سارے': 596,\n 'ڈال': 597,\n 'میں،': 598,\n 'سورج': 599,\n 'لمبا': 600,\n 'قینچی': 601,\n 'اٹھا': 602,\n 'چاکلیٹ': 603,\n 'کشتیاں': 604,\n 'کارکن': 605,\n 'ریس': 606,\n 'گئے': 607,\n 'پہاڑوں': 608,\n 'پیزا۔': 609,\n 'موم': 610,\n 'چڑھ': 611,\n 'فام': 612,\n 'گلے': 613,\n 'مٹی': 614,\n 'پلیٹوں': 615,\n 'لٹک': 616,\n 'لگتا': 617,\n 'لکھا': 618,\n 'بار': 619,\n 'پیالہ': 620,\n 'اسٹور': 621,\n 'سائیڈ': 622,\n 'اسٹیج': 623,\n 'بھیڑیں': 624,\n 'میٹر': 625,\n 'وردی': 626,\n 'کرسیوں': 627,\n 'عورتیں': 628,\n 'کنکریٹ': 629,\n 'ڈبے': 630,\n 'ندی': 631,\n 'پتوں': 632,\n 'تیر': 633,\n 'سالگرہ': 634,\n 'آؤٹ': 635,\n 'پتنگیں': 636,\n 'ترتیب': 637,\n 'سطح': 638,\n 'ہاتھوں': 639,\n 'دانت': 640,\n 'مائکروویو': 641,\n 'پیٹھ': 642,\n 'گچھا': 643,\n 'تھی۔': 644,\n 'وہاں': 645,\n 'گھٹنے': 646,\n 'سویٹر': 647,\n 'کتابوں': 648,\n 'مائیکروفون': 649,\n 'ہیڈ': 650,\n 'پشت': 651,\n 'کمرہ۔': 652,\n 'عوامی': 653,\n 'اکیلا': 654,\n 'سرسبز': 655,\n 'ڈھکا': 656,\n 'گول': 657,\n 'ڈش': 658,\n 'پتھروں': 659,\n 'خلاف': 660,\n 'براؤن': 661,\n 'چشمے': 662,\n 'پچ': 663,\n 'مشروبات': 664,\n 'پٹری': 665,\n 'بھی': 666,\n 'کشتیوں': 667,\n 'شاخ': 668,\n 'تازہ': 669,\n 'داڑھی': 670,\n 'شکل': 671,\n 'اسکی': 672,\n 'کیمرہ': 673,\n 'تھے۔': 674,\n 'باسکٹ': 675,\n 'ماؤس': 676,\n 'گہرے': 677,\n 'بائیک': 678,\n 'ہر': 679,\n 'گاجر': 680,\n 'گرم': 681,\n 'سونے': 682,\n 'چہرہ': 683,\n 'میز۔': 684,\n 'چھڑی': 685,\n 'دیواروں': 686,\n 'چلتی': 687,\n 'گروہ': 688,\n 'سائن': 689,\n 'ماں': 690,\n 'جہاں': 691,\n 'تمام': 692,\n 'پرندوں': 693,\n 'دستانے': 694,\n 'قالین': 695,\n 'لگائے': 696,\n 'جسے': 697,\n 'متعدد': 698,\n 'رکھتا': 699,\n 'گیا۔': 700,\n 'پہنچ': 701,\n 'کچی': 702,\n 'بعد': 703,\n 'نمائش': 704,\n 'کھیلنے': 705,\n 'اچھی': 706,\n 'موجود': 707,\n 'پہلو': 708,\n 'جاتی': 709,\n 'پتلون': 710,\n 'گھور': 711,\n 'خشک': 712,\n 'بینڈ': 713,\n 'ٹینک': 714,\n 'کپڑوں': 715,\n 'گرے': 716,\n 'آتا': 717,\n 'سائز': 718,\n 'کتا۔': 719,\n 'دھات': 720,\n 'رنگوں': 721,\n 'اسکول': 722,\n 'کنٹرولر': 723,\n 'سلاد': 724,\n 'دائیں': 725,\n 'جدید': 726,\n 'سخت': 727,\n 'ٹوتھ': 728,\n 'کتوں': 729,\n 'لینے': 730,\n 'دیتی': 731,\n 'امریکی': 732,\n 'کریم': 733,\n 'مجسمے': 734,\n 'انجن': 735,\n 'بائیں': 736,\n 'پودوں': 737,\n 'ٹکڑوں': 738,\n 'دیا': 739,\n 'بنیان': 740,\n 'لیٹ': 741,\n 'آئینہ': 742,\n 'الماریاں': 743,\n 'غیر': 744,\n 'تھیلے': 745,\n 'برتنوں': 746,\n 'مچھلی': 747,\n 'تصاویر': 748,\n 'کھڑکیوں': 749,\n 'کیلا': 750,\n 'کھاتے': 751,\n 'میزوں': 752,\n 'تکیے': 753,\n 'کنٹرول': 754,\n 'لگی': 755,\n 'گودی': 756,\n 'ہوٹل': 757,\n 'مدد': 758,\n 'سٹی': 759,\n 'خاندان': 760,\n 'مکمل': 761,\n 'گیئر': 762,\n 'گھڑی۔': 763,\n 'مشین': 764,\n 'شادی': 765,\n 'گایوں': 766,\n 'بس۔': 767,\n 'نکل': 768,\n 'درمیانی': 769,\n 'آگ': 770,\n 'پول': 771,\n 'پکا': 772,\n 'خود': 773,\n 'لڑکوں': 774,\n 'سوئنگ': 775,\n 'چولہا': 776,\n 'لگے': 777,\n 'غروب': 778,\n 'گرافٹی': 779,\n 'فوجی': 780,\n 'ہنس': 781,\n 'فرائز': 782,\n 'مجسمہ': 783,\n 'میچ': 784,\n 'موسم': 785,\n 'جھاڑیوں': 786,\n 'گھرا': 787,\n 'کٹے': 788,\n 'اپ۔': 789,\n 'دیکھنے': 790,\n 'پوسٹ': 791,\n 'تار': 792,\n 'بسیں': 793,\n 'سیمنٹ': 794,\n 'دروازہ': 795,\n 'زرافوں': 796,\n 'گرل': 797,\n 'نوجوانوں': 798,\n 'تقریب': 799,\n 'قلم': 800,\n 'شاداب': 801,\n 'لات': 802,\n 'چاروں': 803,\n 'پھینکنے': 804,\n 'بیر': 805,\n 'تیز': 806,\n 'رسی': 807,\n 'لیمپ': 808,\n 'دفتر': 809,\n 'چینی': 810,\n 'مل': 811,\n 'طیارے': 812,\n 'ہلکے': 813,\n 'رقص': 814,\n 'جاتے': 815,\n 'گا': 816,\n 'ریستوران': 817,\n 'والوں': 818,\n 'لان': 819,\n 'آرائشی': 820,\n 'آئس': 821,\n 'پڑی': 822,\n 'بوڑھے': 823,\n 'پرفارم': 824,\n 'اسکرٹ': 825,\n 'گاڑیوں': 826,\n 'پیشہ': 827,\n 'مخالف': 828,\n 'اسنو': 829,\n 'ہو۔': 830,\n 'کانٹے': 831,\n 'آرٹ': 832,\n 'میں۔': 833,\n 'ابھی': 834,\n 'بیک': 835,\n 'باتیں': 836,\n 'عمر': 837,\n 'دانتوں': 838,\n 'گود': 839,\n 'سنک،': 840,\n 'ہائیڈرنٹ۔': 841,\n 'منسلک': 842,\n 'کرسمس': 843,\n 'بچھا': 844,\n 'اڑاتے': 845,\n 'پگڈنڈی': 846,\n 'صوفہ': 847,\n 'دیتا': 848,\n 'کٹنگ': 849,\n 'پودے': 850,\n 'بلیاں': 851,\n 'مسافروں': 852,\n 'سگریٹ': 853,\n 'ایسا': 854,\n 'نمبر': 855,\n 'پہنے،': 856,\n 'نکال': 857,\n 'لٹکی': 858,\n 'ریک': 859,\n 'فاصلے': 860,\n 'فلیٹ': 861,\n 'پک': 862,\n 'سجا': 863,\n 'بچی': 864,\n 'آس': 865,\n 'پوش': 866,\n 'ٹاپنگز': 867,\n 'کھلونے': 868,\n 'پٹی': 869,\n 'گیلے': 870,\n 'ریتیلے': 871,\n 'کم': 872,\n 'ٹوائلٹ۔': 873,\n 'گندے': 874,\n 'مشق': 875,\n 'چرچ': 876,\n 'پیلا': 877,\n 'جہاز۔': 878,\n 'ٹماٹر': 879,\n 'وائی': 880,\n 'آدھا': 881,\n 'وین': 882,\n 'تھامے': 883,\n 'مسکراتا': 884,\n 'تیراکی': 885,\n 'فروخت': 886,\n 'ستھرا': 887,\n 'پہلے': 888,\n 'عجیب': 889,\n 'جیسا': 890,\n 'ڈور': 891,\n 'اسکارف': 892,\n 'چراگاہ': 893,\n 'چمڑے': 894,\n 'فریج': 895,\n 'جھولے': 896,\n 'ہوئے،': 897,\n 'آلو': 898,\n 'آتی': 899,\n 'عورت،': 900,\n 'کاٹنے': 901,\n 'دھکیل': 902,\n 'علاقہ': 903,\n 'مسکراتی': 904,\n 'آدھے': 905,\n 'کرب': 906,\n 'پیچھا': 907,\n 'الخلاء': 908,\n 'جوڑا۔': 909,\n 'پڑے': 910,\n 'حفاظتی': 911,\n 'پردے': 912,\n 'سویٹ': 913,\n 'بیل': 914,\n 'پیسٹری': 915,\n 'سیڑھی': 916,\n 'باغ': 917,\n 'بیڈروم': 918,\n 'تصویریں': 919,\n 'ایریا': 920,\n 'قطبی': 921,\n 'تنگ': 922,\n 'ریلنگ': 923,\n 'کیے': 924,\n 'مین': 925,\n 'پر۔': 926,\n 'سوئمنگ': 927,\n 'طرز': 928,\n 'چھ': 929,\n 'کیچر': 930,\n 'پلیٹیں': 931,\n 'پیڈل': 932,\n 'جزوی': 933,\n 'افریقی': 934,\n 'خوش': 935,\n 'بیٹھتے': 936,\n 'وسط': 937,\n 'آف': 938,\n 'پینٹنگ': 939,\n 'بنایا': 940,\n 'کندھے': 941,\n 'دوست': 942,\n 'سجاوٹ': 943,\n 'اسکیئنگ': 944,\n 'صرف': 945,\n 'ور': 946,\n 'عینک': 947,\n 'کھلاڑیوں': 948,\n '2': 949,\n 'گلدستے': 950,\n 'عبور': 951,\n 'وہیل': 952,\n 'شہری': 953,\n 'بچہ۔': 954,\n 'آفتاب': 955,\n 'چمچ': 956,\n 'ٹوپیاں': 957,\n 'چمنی': 958,\n 'چٹنی': 959,\n 'اونچا': 960,\n 'غول': 961,\n 'برنگی': 962,\n 'پکنک': 963,\n 'گھڑا': 964,\n 'پینے': 965,\n 'گھری': 966,\n 'ہوئے۔': 967,\n 'آنکھوں': 968,\n 'جرسی': 969,\n 'چیک': 970,\n 'چٹانوں': 971,\n 'گندا': 972,\n 'بلیک': 973,\n 'موڑ': 974,\n 'مسکراتے': 975,\n 'پرس': 976,\n 'شاپنگ': 977,\n 'رکھ': 978,\n 'مشروب': 979,\n 'پارٹی': 980,\n 'فوڈ': 981,\n 'پرواز': 982,\n 'قدمی': 983,\n 'اسکیئرز': 984,\n 'کنٹری': 985,\n 'ہری': 986,\n 'جھنڈ': 987,\n 'ڈھانچے': 988,\n 'ٹانگوں': 989,\n 'شاٹ': 990,\n 'بنی': 991,\n 'پیالہ۔': 992,\n 'چہل': 993,\n 'لڑکیوں': 994,\n 'ترتیبی': 995,\n 'جوتوں': 996,\n 'کان': 997,\n 'چاول': 998,\n 'رکاوٹ': 999,\n 'تماشائیوں': 1000,\n ...}"},"metadata":{}}]},{"cell_type":"markdown","source":"This is dictionary containing urdu words as keys and their corresponding indices as values. It represents the vocabulary learned by the tokenizer during the fitting process","metadata":{}},{"cell_type":"code","source":"# get maximum length of the caption available\nmax_length = max(len(ucap.split()) for ucap in data['urdu_caption'])\nmax_length","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:35.169924Z","iopub.execute_input":"2024-07-29T16:29:35.170268Z","iopub.status.idle":"2024-07-29T16:29:35.762790Z","shell.execute_reply.started":"2024-07-29T16:29:35.170243Z","shell.execute_reply":"2024-07-29T16:29:35.761917Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"94"},"metadata":{}}]},{"cell_type":"markdown","source":"## Splitting the Data","metadata":{}},{"cell_type":"markdown","source":"5 Captions per image\n* Train = 70% (562089)\n* Validation = 20% (140523)\n* Test = 10% (78069)\n\n1 caption per image:\n* Train = 72% (112370)\n* Validation = 18% (28093)\n* Test = 10% (15607)","metadata":{}},{"cell_type":"code","source":"X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(data['image'], data['urdu_caption'], \n                                                                test_size = 0.10, random_state = 30, shuffle = True)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_train_valid, Y_train_valid, \n                                                      test_size = 0.2, random_state = 30, shuffle = True)\n\nprint('Train set size:', X_train.shape)\nprint('Validation set size:', X_valid.shape)\nprint('Test set size:', X_test.shape)\nprint('Total data size', X_train.shape[0] + X_valid.shape[0] + X_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:35.763792Z","iopub.execute_input":"2024-07-29T16:29:35.764028Z","iopub.status.idle":"2024-07-29T16:29:35.918941Z","shell.execute_reply.started":"2024-07-29T16:29:35.764006Z","shell.execute_reply":"2024-07-29T16:29:35.918009Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Train set size: (337111,)\nValidation set size: (84278,)\nTest set size: (46821,)\nTotal data size 468210\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"markdown","source":"* A model architecture that will combine visual information from images with textual information from partial captions to generate captions for images.\n* The model will learn to generate captions based on both the visual content and the context provided by the captions.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # image feature extractor model\n    inputs1 = Input(shape = (2048,))             # defines an input layer for the image features.\n    fe1 = BatchNormalization()(inputs1)          # applies batch normalization to the input image features.\n    fe2 = Dense(512, activation = 'relu')(fe1)   # applies a dense layer with 512 units and ReLU activation to the batch-normalized image features.\n\n    # partial caption sequence model\n    inputs2 = Input(shape = (max_length,))       # defines an input layer for the partial caption sequences.\n    se1 = Embedding(vocab_size, 512)(inputs2)   # embeds the input sequences into dense vectors of size 512. \n                                                 # this layer uses an embedding matrix with a vocabulary size of vocab_size.\n    se2 = BatchNormalization()(se1)              # applies batch normalization to the embedded sequences.\n    se3 = LSTM(256)(se2)                         # applies a LSTM layer with 256 units to the batch-normalized embedded sequences.\n\n    # decoder model\n    decoder = Concatenate()([fe2, se3])                  # concatenates output features from the image extractor and partial caption sequence models.\n    decoder2 = Dense(512, activation = 'relu')(decoder)  # applies a dense layer with 512 units and ReLU activation to the concatenated features.\n    outputs = Dense(vocab_size,                          # applies a dense layer with vocab_size units and softmax activation to produce the output\n                    activation = 'softmax')(decoder2)    # probability distribution over the vocabulary.\n\n    # merge 2 networks\n    model = Model(inputs = [inputs1, inputs2], outputs = outputs)\n\n    optimizer = Adam(learning_rate = 0.005,\n                     clipvalue = 5.0)          # technique used to limit the magnitude of gradients during training.\n                                               # helps stabilize the training process, especially with exploding gradients.\n\n    model.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n                  optimizer = optimizer,               # used for updating the weights.\n                  metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:55:31.179268Z","iopub.execute_input":"2024-06-04T13:55:31.179530Z","iopub.status.idle":"2024-06-04T13:55:31.777113Z","shell.execute_reply.started":"2024-06-04T13:55:31.179508Z","shell.execute_reply":"2024-06-04T13:55:31.776150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The merged model combines the image feature extractor model and the partial caption sequence model into a single model.<br>\nIt takes both the image features (inputs1) and the partial caption sequences (inputs2) as inputs and produces the output probabilities over the vocabulary (outputs).","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:55:31.780014Z","iopub.execute_input":"2024-06-04T13:55:31.780285Z","iopub.status.idle":"2024-06-04T13:55:31.806506Z","shell.execute_reply.started":"2024-06-04T13:55:31.780263Z","shell.execute_reply":"2024-06-04T13:55:31.805701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:55:31.808007Z","iopub.execute_input":"2024-06-04T13:55:31.808374Z","iopub.status.idle":"2024-06-04T13:55:32.159837Z","shell.execute_reply.started":"2024-06-04T13:55:31.808345Z","shell.execute_reply":"2024-06-04T13:55:32.158981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# create data generator to get data in batch (avoids session crash)\ndef data_generator(data, features, tokenizer, max_length, vocab_size, batch_size):\n    # loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data['image'].tolist():\n            n += 1\n            captions = data[data['image'] == key]['urdu_caption']   # ek img k sary captions, 'captions' is a list\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pairs\n                    in_seq, out_seq = seq[:i], seq[i]    # for predicting next word for a given word\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]   # to make length of all captions same by appending zeros\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]   # to tell the index where the word is stored in tokenizer\n                    \n                    # store the sequences (all these are 2D lists now)\n                    X1.append(features[key.split('.')[0]][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield (X1, X2), y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:55:32.160839Z","iopub.execute_input":"2024-06-04T13:55:32.161122Z","iopub.status.idle":"2024-06-04T13:55:32.170604Z","shell.execute_reply.started":"2024-06-04T13:55:32.161099Z","shell.execute_reply":"2024-06-04T13:55:32.169750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = {'image': X_train.tolist(), 'urdu_caption': Y_train.tolist()}\ntrain_set = pd.DataFrame(train_set)\ntrain_set","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:35.920153Z","iopub.execute_input":"2024-07-29T16:29:35.920443Z","iopub.status.idle":"2024-07-29T16:29:36.058314Z","shell.execute_reply.started":"2024-07-29T16:29:35.920418Z","shell.execute_reply":"2024-07-29T16:29:36.057443Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                image                                       urdu_caption\n0       img112934.jpg  endseq گائیوں کا ایک غول خاردار باڑ کے پاس کھڑ...\n1       img062210.jpg  endseq یہ ایک پل کے بارے میں انتباہ کے نشان کی...\n2       img015619.jpg  endseq لوگ میز کے پاس بیٹھے مسکرا رہے ہیں اور ...\n3       img060858.jpg  endseq پٹریوں پر نارنجی پٹی والی سفید ٹرین sta...\n4       img128054.jpg  endseq پارک میں ایک سیاہ اور سفید کتا برف میں ...\n...               ...                                                ...\n337106  img070574.jpg  endseq مردوں کا ایک گروپ تلوار سے سفید چادر کا...\n337107  img046623.jpg  endseq ایک آدمی لمبی بازو کی قمیض پہنے سمارٹ ف...\n337108  img007919.jpg  endseq لوگوں کا ایک گروپ زرافے کے گرد کھڑا ہے۔...\n337109  img059922.jpg  endseq ایک میز کے اوپر چمکدار ڈونٹس سے بھرے تی...\n337110  img071134.jpg  endseq یہ ماں خوش ہے کہ اس کے بیٹے اور بیٹی کو...\n\n[337111 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>urdu_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img112934.jpg</td>\n      <td>endseq گائیوں کا ایک غول خاردار باڑ کے پاس کھڑ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img062210.jpg</td>\n      <td>endseq یہ ایک پل کے بارے میں انتباہ کے نشان کی...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img015619.jpg</td>\n      <td>endseq لوگ میز کے پاس بیٹھے مسکرا رہے ہیں اور ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img060858.jpg</td>\n      <td>endseq پٹریوں پر نارنجی پٹی والی سفید ٹرین sta...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img128054.jpg</td>\n      <td>endseq پارک میں ایک سیاہ اور سفید کتا برف میں ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>337106</th>\n      <td>img070574.jpg</td>\n      <td>endseq مردوں کا ایک گروپ تلوار سے سفید چادر کا...</td>\n    </tr>\n    <tr>\n      <th>337107</th>\n      <td>img046623.jpg</td>\n      <td>endseq ایک آدمی لمبی بازو کی قمیض پہنے سمارٹ ف...</td>\n    </tr>\n    <tr>\n      <th>337108</th>\n      <td>img007919.jpg</td>\n      <td>endseq لوگوں کا ایک گروپ زرافے کے گرد کھڑا ہے۔...</td>\n    </tr>\n    <tr>\n      <th>337109</th>\n      <td>img059922.jpg</td>\n      <td>endseq ایک میز کے اوپر چمکدار ڈونٹس سے بھرے تی...</td>\n    </tr>\n    <tr>\n      <th>337110</th>\n      <td>img071134.jpg</td>\n      <td>endseq یہ ماں خوش ہے کہ اس کے بیٹے اور بیٹی کو...</td>\n    </tr>\n  </tbody>\n</table>\n<p>337111 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 60\nsteps = len(X_train) // batch_size\n\n#Define a ModelCheckpoint callback\n#Callbacks provide flexibility and customization to the training process,\ncheckpoint_filepath = '/kaggle/working/model_checkpoint.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath = checkpoint_filepath,\n    save_weights_only = False,\n    monitor = 'val_loss',\n    mode = 'min',\n    save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n    \n    # fit for one epoch\n    model.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1, callbacks = [model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:55:43.778730Z","iopub.execute_input":"2024-06-04T13:55:43.779568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('Trained_Xception_LSTM_epoch10.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:36.059277Z","iopub.execute_input":"2024-07-29T16:29:36.059519Z","iopub.status.idle":"2024-07-29T16:29:36.063778Z","shell.execute_reply.started":"2024-07-29T16:29:36.059498Z","shell.execute_reply":"2024-07-29T16:29:36.062771Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model2 = load_model('/kaggle/input/modelepoch5/Trained_Xception_LSTM_epoch5.h5')\n\noptimizer = Adam(learning_rate = 0.005,\n                     clipvalue = 5.0)          # technique used to limit the magnitude of gradients during training.\n                                               # helps stabilize the training process, especially with exploding gradients.\n\nmodel2.compile(loss = \"categorical_crossentropy\",   # loss function commonly used for multi-class classification problems.\n              optimizer = optimizer,               # used for updating the weights.\n              metrics = ['accuracy'])              # evaluation metric to monitor during training.","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:01:42.486476Z","iopub.execute_input":"2024-06-02T09:01:42.487034Z","iopub.status.idle":"2024-06-02T09:01:44.527695Z","shell.execute_reply.started":"2024-06-02T09:01:42.487004Z","shell.execute_reply":"2024-06-02T09:01:44.526865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:01:46.937555Z","iopub.execute_input":"2024-06-02T09:01:46.937942Z","iopub.status.idle":"2024-06-02T09:01:46.965652Z","shell.execute_reply.started":"2024-06-02T09:01:46.937914Z","shell.execute_reply":"2024-06-02T09:01:46.964756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 1\nbatch_size = 40\nsteps = len(X_train) // batch_size\n\n#Define a ModelCheckpoint callback\n#Callbacks provide flexibility and customization to the training process,\n# checkpoint_filepath = '/kaggle/working/model_checkpoint.keras'\n# model_checkpoint_callback = ModelCheckpoint(\n#     filepath = checkpoint_filepath,\n#     save_weights_only = False,\n#     monitor = 'val_loss',\n#     mode = 'min',\n#     save_best_only = True)\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train_set, features, tokenizer, max_length, vocab_size, batch_size)\n    \n    # fit for one epoch\n    model2.fit(generator, epochs = 1, steps_per_epoch = steps, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:01:51.785899Z","iopub.execute_input":"2024-06-02T09:01:51.786282Z","iopub.status.idle":"2024-06-02T09:04:42.816145Z","shell.execute_reply.started":"2024-06-02T09:01:51.786253Z","shell.execute_reply":"2024-06-02T09:04:42.813652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.save('Trained_Xception_LSTM_1c6e')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate a Caption using the Model","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word,index, in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:42.501047Z","iopub.execute_input":"2024-07-29T16:29:42.501412Z","iopub.status.idle":"2024-07-29T16:29:42.506276Z","shell.execute_reply.started":"2024-07-29T16:29:42.501383Z","shell.execute_reply":"2024-07-29T16:29:42.505345Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# generate caption for an image\ndef predict_caption(model, image, tokenizer, max_length):\n    # add start tag for generation process\n    in_text = 'endseq'\n    # iterate over the max length of sequence\n    for i in range(max_length):\n        # encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        \n        # pad the sequence\n        sequence = pad_sequences([sequence], max_length)\n        \n        # predict next word\n        yhat = model.predict([image, sequence], verbose=0)\n        \n        # get index with high probability\n        yhat = np.argmax(yhat)\n        \n        # convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        \n        # stop if word not found\n        if word is None:\n            break\n        # append word as input for generating next word\n        in_text += \" \" + word\n        \n        # stop if we reach end tag\n        if word == 'startseq':\n            break\n    return in_text","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:54.049008Z","iopub.execute_input":"2024-07-29T16:29:54.049386Z","iopub.status.idle":"2024-07-29T16:29:54.058470Z","shell.execute_reply.started":"2024-07-29T16:29:54.049356Z","shell.execute_reply":"2024-07-29T16:29:54.057509Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"m1 = load_model('/kaggle/input/bleu-test/Trained_YOLO_LSTM_3c10e.h5')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:20:36.947748Z","iopub.execute_input":"2024-07-29T19:20:36.948129Z","iopub.status.idle":"2024-07-29T19:20:38.512343Z","shell.execute_reply.started":"2024-07-29T19:20:36.948084Z","shell.execute_reply":"2024-07-29T19:20:38.511489Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"m2 = load_model('/kaggle/input/bleu-test/Trained_YOLO_GRU_3c7e.h5')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:23:48.892592Z","iopub.execute_input":"2024-07-29T19:23:48.893361Z","iopub.status.idle":"2024-07-29T19:23:50.063396Z","shell.execute_reply.started":"2024-07-29T19:23:48.893323Z","shell.execute_reply":"2024-07-29T19:23:50.062430Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:30:34.225999Z","iopub.execute_input":"2024-07-29T16:30:34.226382Z","iopub.status.idle":"2024-07-29T16:30:48.650356Z","shell.execute_reply.started":"2024-07-29T16:30:34.226351Z","shell.execute_reply":"2024-07-29T16:30:48.649168Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.2.68-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m758.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.2.68-py3-none-any.whl (828 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.2.68 ultralytics-thop-2.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:30:48.652912Z","iopub.execute_input":"2024-07-29T16:30:48.653419Z","iopub.status.idle":"2024-07-29T16:30:51.846034Z","shell.execute_reply.started":"2024-07-29T16:30:48.653379Z","shell.execute_reply":"2024-07-29T16:30:51.845157Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"cnn = YOLO(\"yolov8m-cls.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:30:51.847155Z","iopub.execute_input":"2024-07-29T16:30:51.848060Z","iopub.status.idle":"2024-07-29T16:30:54.144001Z","shell.execute_reply.started":"2024-07-29T16:30:51.848032Z","shell.execute_reply":"2024-07-29T16:30:54.143020Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-cls.pt to 'yolov8m-cls.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 32.7M/32.7M [00:00<00:00, 87.5MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# validate with test data\n# with strategy.scope():\nactual, predicted = list(), list()\n\nfor key in tqdm(X_test.iloc[:2000]):\n    # get actual caption\n    image = '/kaggle/input/images/Images/'+key\n    captions = data[data['image'] == key]['urdu_caption']\n    features = cnn.predict(image, embed = [-1])\n\n    # predict the caption for image\n    y_pred = predict_caption(m1, features, tokenizer, max_length) \n\n    # split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n\n    # append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n\n# calcuate BLEU score\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(0.7, 0.2, 0.1)))\n# print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:26:00.270853Z","iopub.execute_input":"2024-07-29T19:26:00.271250Z","iopub.status.idle":"2024-07-29T19:29:48.090965Z","shell.execute_reply.started":"2024-07-29T19:26:00.271219Z","shell.execute_reply":"2024-07-29T19:29:48.090005Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237653ce545240108d1689b6c0a18ccb"}},"metadata":{}},{"name":"stdout","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBLEU-1: 0.321402\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_caption(image_name):\n    # load the image\n    image_id = image_name.split('.')[0]\n    img_path = \"/kaggle/input/images/Images/\" + image_name\n    image = Image.open(img_path)\n\n    # Predict caption for image\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)[8:][:-6]\n    print('--------------------Predicted--------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"img000000.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}